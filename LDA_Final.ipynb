{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as mltp\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "import seaborn as sea\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import pickle \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "from typing import Dict, List\n",
    "import spacy\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_markdown_file(file_path):\n",
    "    with open(file_path, \"r\") as stream:\n",
    "        markdown_str = stream.read()\n",
    "        return markdown_str\n",
    "\n",
    "def load_yaml_file(file_path):\n",
    "    # reads the yml files as a dictionary, were each topic is a key and the values are a list of elements\n",
    "    with open(file_path, \"r\") as stream:\n",
    "        yaml_dict = yaml.safe_load(stream)\n",
    "        return yaml_dict\n",
    "\n",
    "\n",
    "\n",
    "def _add_sentence_to_list(sentence: str, sentences_list):\n",
    "    \"\"\"\n",
    "    Add a sentence to the list of sentences.\n",
    "    Args:\n",
    "        sentence (str):\n",
    "            Sentence to be added.\n",
    "        sentences (List[str]):\n",
    "            List of sentences.\n",
    "    \"\"\"\n",
    "    while sentence.startswith(\" \"):\n",
    "        # remove leading space\n",
    "        sentence = sentence[1:]\n",
    "    if all(c in punctuation for c in sentence) or len(sentence) == 1:\n",
    "        # skip sentences with only punctuation\n",
    "        return\n",
    "    sentences_list.append(sentence)\n",
    "\n",
    "\n",
    "def get_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Get sentences from a text.\n",
    "    Args:\n",
    "        text (str):\n",
    "            Text to be processed.\n",
    "    Returns:\n",
    "        List[str]:\n",
    "            List of sentences.\n",
    "    \"\"\"\n",
    "    # get the paragraphs\n",
    "    text=   re.sub(\" \\d+\\n\", \".\", text)\n",
    "    text=   re.sub(\"\\n\\d+\", \" \", text)\n",
    "    text=   re.sub(\"\\n\", \" \", text)\n",
    "    text=   re.sub(\"\\d+.\", \"\", text)\n",
    "    paragraphs = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "    paragraphs = [p for p in paragraphs if p != \"\"]\n",
    "    # get the sentences from the paragraphs\n",
    "    sentences = list()\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.startswith(\"#\"):\n",
    "            _add_sentence_to_list(paragraph, sentences)\n",
    "            continue\n",
    "        prev_sentence_idx = 0\n",
    "        for idx in range(len(paragraph)):\n",
    "            if idx + 1 < len(paragraph):\n",
    "                if (paragraph[idx] == \".\" and not paragraph[idx + 1].isdigit()) or (\n",
    "                    paragraph[idx] in \"!?\"\n",
    "                ):\n",
    "                    sentence = paragraph[prev_sentence_idx : idx + 1]\n",
    "                    _add_sentence_to_list(sentence, sentences)\n",
    "                    prev_sentence_idx = idx + 1\n",
    "            else:\n",
    "                sentence = paragraph[prev_sentence_idx:]\n",
    "                _add_sentence_to_list(sentence, sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='data/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_names = os.listdir(DATA_DIR)\n",
    "data_names = [name[:-4] for name in data_names if name != \".DS_Store\"]\n",
    "data_names = list(filter(None, data_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIE_GRUENEN_Wahlprogramm_2021',\n",
       " 'FDP_Wahlprogramm_2021',\n",
       " 'DIE_LINKE_Wahlprogramm_2021',\n",
       " 'CDU-CSU_Wahlrprogramm_2021',\n",
       " 'SPD_Wahlprogramm_2021',\n",
       " 'AFD_Wahlprogramm_2021']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85208f5b1a54aa68bbcb0e275f1b3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_liste=[]\n",
    "dic_list={}\n",
    "for element in tqdm(list(filter(None, data_names))):\n",
    "    program_txt = load_markdown_file(f\"data/raw/{element}.txt\")\n",
    "    sentences = get_sentences(program_txt)\n",
    "    for item in sentences:\n",
    "        text_liste.append(item)\n",
    "    dic_list[element]=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IEITTESALTENTR Alles ist drin'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_liste[0][0:90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy ist eine beliebte Open-Source-Bibliothek für die Verarbeitung natürlicher Sprache (NLP) in Python. Sie enthält unter anderem Werkzeuge für Tokenisierung, Part-of-Speech-Tagging, Dependency Parsing und Named Entity Recognition. Diese Werkzeuge sind in einer Pipeline organisiert, die den Eingabetext verarbeitet und eine Vielzahl von linguistischen Merkmalen extrahiert, die für eine Vielzahl von NLP-Aufgaben verwendet werden können.\n",
    "\n",
    "Der erste Schritt in der SpaCy-Pipeline ist die Tokenisierung, d. h. die Zerlegung des Eingabetextes in einzelne Wörter (Token). Dies geschieht anhand einer Reihe von Regeln, die Interpunktion, Kontraktionen und andere Sonderfälle berücksichtigen. Die so entstandenen Token werden dann mit verschiedenen linguistischen Merkmalen versehen, z. B. mit ihrem Part-of-Speech-Tag und ihrem Lemma (Grundform).\n",
    "\n",
    "Der nächste Schritt in der Pipeline ist das Part-of-Speech-Tagging, bei dem jedes Token mit seinem Part of Speech (z. B. Substantiv, Verb oder Adjektiv) gekennzeichnet wird. Dies geschieht in der Regel mithilfe eines maschinellen Lernmodells, das auf einem großen annotierten Textkorpus trainiert wurde. Das Part-of-Speech-Tagging ist ein wichtiger Schritt, da es dem Modell ermöglicht, die grammatikalische Rolle jedes Wortes im Satz zu identifizieren, was für Aufgaben wie Dependency Parsing und Named Entity Recognition nützlich sein kann.\n",
    "\n",
    "Beim Dependency Parsing werden die Beziehungen zwischen Wörtern in einem Satz identifiziert, z. B. Subjekt-Verben-Beziehungen und Modifikator-Kopf-Beziehungen. Dies geschieht in der Regel mithilfe eines maschinellen Lernmodells, das auf einem großen annotierten Textkorpus trainiert wurde. Das Parsen von Abhängigkeiten ist ein wichtiger Schritt, da es dem Modell ermöglicht, die Struktur des Satzes und die Beziehungen zwischen den Wörtern zu verstehen, was für Aufgaben wie die Informationsextraktion und die maschinelle Übersetzung nützlich sein kann. Jedoch wird dies Aufgrund der Anwendung ncith verwendet, da dies meist mit den höchsten Aufwand erzeugt\n",
    "\n",
    "Bei der Erkennung benannter Entitäten handelt es sich um den Prozess der Identifizierung und Klassifizierung benannter Entitäten in Texten, z. B. Personen, Organisationen und Orte. Dies geschieht in der Regel mithilfe eines maschinellen Lernmodells, das auf einem großen kommentierten Textkorpus trainiert wurde. Die Erkennung von benannten Entitäten ist ein wichtiger Schritt, da sie es dem Modell ermöglicht, strukturierte Informationen aus unstrukturiertem Text zu extrahieren, was für Aufgaben wie die Informationsextraktion und die Textklassifizierung nützlich sein kann.\n",
    "\n",
    "Insgesamt ist die SpaCy-Pipeline ein leistungsfähiges Werkzeug zur Extraktion einer Vielzahl von linguistischen Merkmalen aus Text, die für eine Vielzahl von NLP-Aufgaben verwendet werden können. Durch die Kombination dieser Merkmale mit Modellen des maschinellen Lernens ist es möglich, anspruchsvolle NLP-Anwendungen zu erstellen, die natürlichsprachliche Texte analysieren und verstehen können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_test(texts,  allowed_posttags=['NOUN','ADJ','VERB','ADV']):\n",
    "    nlp=spacy.load('en_core_web_md',disable=['parser','ner'])\n",
    "    texts_out=[]\n",
    "    for text in tqdm(texts):\n",
    "        doc= nlp(text)\n",
    "        new_text=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_posttags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final=' '.join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad816fded9ac49319f0c8dc89ba350e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemma_text=lemmatization_test(text_liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ist drin\n"
     ]
    }
   ],
   "source": [
    "print(lemma_text[0][0:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final=[]\n",
    "    for text in tqdm(texts):\n",
    "        new= gensim.utils.simple_preprocess(text,deacc=True)\n",
    "        final.append(new)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92090784f0974b80ac499c2b52fe5a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_words=gen_words(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ist', 'drin']\n"
     ]
    }
   ],
   "source": [
    "print(data_words[0][0:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein n-Gramm ist eine zusammenhängende Sequenz von n Elementen aus einer bestimmten Text- oder Sprachprobe. N-Gramme sind in der natürlichen Sprachverarbeitung und Computerlinguistik weit verbreitet. Sie können für eine Vielzahl von Aufgaben verwendet werden, z. B. für die Sprachmodellierung, die Informationsbeschaffung und die maschinelle Übersetzung.\n",
    "\n",
    "Bigramme sind Zwei-Wort-Folgen von Wörtern, die nacheinander in einem Text vorkommen. In dem Satz \"die Katze im Hut\" wären die Bigramme zum Beispiel \"die Katze\", \"Katze in\" und \"in der\". Bigramme können für Aufgaben wie Rechtschreibkorrektur, Sprachmodellierung und Informationsabfrage nützlich sein.\n",
    "\n",
    "Trigramme sind Drei-Wort-Folgen von Wörtern, die nacheinander in einem Text vorkommen. In dem Satz \"die Katze im Hut\" wären die Trigramme zum Beispiel \"die Katze in\", \"die Katze im\" und \"im Hut\". Trigramme können für Aufgaben wie Sprachmodellierung und Informationsabfrage nützlich sein.\n",
    "\n",
    "N-Gramme werden häufig in der natürlichen Sprachverarbeitung verwendet, da sie den Kontext und die Bedeutung von Wörtern in einem Text erfassen können. Beispielsweise ist das Bigramm \"New York\" informativer als die einzelnen Wörter \"New\" und \"York\", da es darauf hindeutet, dass die beiden Wörter miteinander verwandt sind und sich wahrscheinlich auf die Stadt beziehen. In ähnlicher Weise ist das Trigramm \"die Katze in\" informativer als die Bigramme \"die Katze\" und \"Katze in\", da es darauf hindeutet, dass die Wörter Teil einer Sequenz sind und wahrscheinlich in irgendeiner Weise miteinander in Beziehung stehen.\n",
    "\n",
    "Insgesamt können n-Gramme ein nützliches Werkzeug für das Verständnis und die Verarbeitung von Sprachdaten sein, und Bigramme und Trigramme werden häufig in einer Vielzahl von Aufgaben der natürlichen Sprachverarbeitung verwendet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases=gensim.models.Phrases(data_words,min_count=3,threshold=100)\n",
    "trigram_phases=gensim.models.Phrases(bigram_phrases[data_words],threshold=50)\n",
    "\n",
    "bigram=gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram=gensim.models.phrases.Phraser(trigram_phases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "def make_trgram(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams=make_bigrams(data_words)\n",
    "data_bigrams_trigrams=make_trgram(data_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1124aa148e4e49a0ab05436decfe425b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word=corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus=[id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[0][0:90])\n",
    "\n",
    "tfidf=TfidfModel(corpus,id2word=id2word)\n",
    "\n",
    "low_value=0.03\n",
    "words=[]\n",
    "words_missing_in_tfdf=[]\n",
    "\n",
    "for i in tqdm(range(0,len(corpus))):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops=low_value_words+words_missing_in_tfdf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]  \n",
    "    corpus[i]=new_bow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein wichtiger aber auch schwieriger Aspekt des LDA prozesses ist die Bestimmung der ANzhal der Topics. Hierbei ist wie bei Maschine Learning eine autonmatische Verbesserung und Lernen nicht gegeben, weshlab durch erforschen und Analysen die Optimalen Parameter gefunden werden müssen. Dies ist besonders schweirig, da meist Klassen entstehen, welche eine hohe Anzahl an Elementen enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wirtschaft',\n",
       " 'Klima',\n",
       " 'Bildung',\n",
       " 'Gesundheit',\n",
       " 'Wissenschaft',\n",
       " 'soziale Ursachen',\n",
       " 'Politik und Ideologie',\n",
       " 'Infrastruktur']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = load_yaml_file('topic_modeling/topic_g.yml')\n",
    "list(topics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=len(list(topics.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=gensim.models.ldamodel.LdaModel(\n",
    "corpus=corpus,\n",
    "id2word=id2word,\n",
    "num_topics=num_topics,\n",
    "random_state=100,\n",
    "update_every=1,\n",
    "chunksize=100,\n",
    "passes=10,\n",
    "alpha='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.7370000000000003, 1: 0.7860000000000003, 2: 0.7230000000000004, 3: 0.6790000000000003, 4: 0.7360000000000003, 5: 0.8840000000000003, 6: 0.7330000000000004, 7: 0.6930000000000004}\n"
     ]
    }
   ],
   "source": [
    "dic={}\n",
    "dic_sum={}\n",
    "for idx, topic in lda_model.print_topics(num_topics,num_words=100):\n",
    "    elements=[]\n",
    "    percent=[]\n",
    "\n",
    "    for e in topic.split('+'):\n",
    "        elements.append(e.split('*')[1].replace('\"','').strip())\n",
    "        percent.append(float(e.split('*')[0].replace('\"','').strip()))\n",
    "\n",
    "    dic[str(idx)+'_word']=elements\n",
    "    dic[str(idx)+'_per']=percent\n",
    "    dic_sum[idx]=sum(percent)\n",
    "print(dic_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur bestimmung der optimalen Anzahl von relevanten Wörtern wurden daher auf 100 gestzt, um nah an 100% zu sein. Da je nach Wort eine untershciedliche Wahrscheinlichkeit und Somit gewichtung für das eine oder andere Topic entsteht\n",
    "\n",
    "Daher werden die Wörter im nahchinein genutzt, um eine zurodnung durch abgleich und multiplikation der Gewichtung zu erziehlen, um basierend davon jenes Topic zu deklarieren, welches am Wahrscheinlichsten ist. Aus diesem Grund dies aus dem Wert der Summe der auftretenden Werte verglichen mit der Summe der Unterschiedlichen Gewichte mäglicher alternastiven Topics\n",
    "Dafür wird volgendes getan:\n",
    "1. Identifizieren, welche Wörter eines Topics und wei oft in einem Text enthalten sind\n",
    "2. Basierend auf diesen die Wahrscheinlihckeiten bestimmen\n",
    "3. Diese Addieren und den höchsten Wert identifizieren, welche das Wahrschienlichste Topic entspricht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_word</th>\n",
       "      <th>0_per</th>\n",
       "      <th>1_word</th>\n",
       "      <th>1_per</th>\n",
       "      <th>2_word</th>\n",
       "      <th>2_per</th>\n",
       "      <th>3_word</th>\n",
       "      <th>3_per</th>\n",
       "      <th>4_word</th>\n",
       "      <th>4_per</th>\n",
       "      <th>5_word</th>\n",
       "      <th>5_per</th>\n",
       "      <th>6_word</th>\n",
       "      <th>6_per</th>\n",
       "      <th>7_word</th>\n",
       "      <th>7_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wollen</td>\n",
       "      <td>0.075</td>\n",
       "      <td>ist</td>\n",
       "      <td>0.184</td>\n",
       "      <td>auch</td>\n",
       "      <td>0.122</td>\n",
       "      <td>starken</td>\n",
       "      <td>0.037</td>\n",
       "      <td>werden</td>\n",
       "      <td>0.147</td>\n",
       "      <td>die</td>\n",
       "      <td>0.215</td>\n",
       "      <td>fur</td>\n",
       "      <td>0.278</td>\n",
       "      <td>dem</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oder</td>\n",
       "      <td>0.069</td>\n",
       "      <td>nicht</td>\n",
       "      <td>0.112</td>\n",
       "      <td>muss</td>\n",
       "      <td>0.080</td>\n",
       "      <td>ihrer</td>\n",
       "      <td>0.034</td>\n",
       "      <td>mussen</td>\n",
       "      <td>0.081</td>\n",
       "      <td>und</td>\n",
       "      <td>0.150</td>\n",
       "      <td>es</td>\n",
       "      <td>0.028</td>\n",
       "      <td>ihre</td>\n",
       "      <td>0.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wird</td>\n",
       "      <td>0.067</td>\n",
       "      <td>nur</td>\n",
       "      <td>0.061</td>\n",
       "      <td>al</td>\n",
       "      <td>0.068</td>\n",
       "      <td>unter</td>\n",
       "      <td>0.026</td>\n",
       "      <td>haben</td>\n",
       "      <td>0.050</td>\n",
       "      <td>der</td>\n",
       "      <td>0.132</td>\n",
       "      <td>unseren</td>\n",
       "      <td>0.020</td>\n",
       "      <td>vor</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wir</td>\n",
       "      <td>0.061</td>\n",
       "      <td>sowie</td>\n",
       "      <td>0.043</td>\n",
       "      <td>un</td>\n",
       "      <td>0.030</td>\n",
       "      <td>ohne</td>\n",
       "      <td>0.026</td>\n",
       "      <td>sollen</td>\n",
       "      <td>0.040</td>\n",
       "      <td>den</td>\n",
       "      <td>0.042</td>\n",
       "      <td>uber</td>\n",
       "      <td>0.019</td>\n",
       "      <td>unterstutzen</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fordern</td>\n",
       "      <td>0.051</td>\n",
       "      <td>zur</td>\n",
       "      <td>0.033</td>\n",
       "      <td>unserer</td>\n",
       "      <td>0.028</td>\n",
       "      <td>deutsche</td>\n",
       "      <td>0.025</td>\n",
       "      <td>soll</td>\n",
       "      <td>0.032</td>\n",
       "      <td>zu</td>\n",
       "      <td>0.039</td>\n",
       "      <td>dazu</td>\n",
       "      <td>0.017</td>\n",
       "      <td>noch</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_word  0_per 1_word  1_per   2_word  2_per    3_word  3_per  4_word  \\\n",
       "0   wollen  0.075    ist  0.184     auch  0.122   starken  0.037  werden   \n",
       "1     oder  0.069  nicht  0.112     muss  0.080     ihrer  0.034  mussen   \n",
       "2     wird  0.067    nur  0.061       al  0.068     unter  0.026   haben   \n",
       "3      wir  0.061  sowie  0.043       un  0.030      ohne  0.026  sollen   \n",
       "4  fordern  0.051    zur  0.033  unserer  0.028  deutsche  0.025    soll   \n",
       "\n",
       "   4_per 5_word  5_per   6_word  6_per        7_word  7_per  \n",
       "0  0.147    die  0.215      fur  0.278           dem  0.110  \n",
       "1  0.081    und  0.150       es  0.028          ihre  0.053  \n",
       "2  0.050    der  0.132  unseren  0.020           vor  0.044  \n",
       "3  0.040    den  0.042     uber  0.019  unterstutzen  0.035  \n",
       "4  0.032     zu  0.039     dazu  0.017          noch  0.028  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic=pd.DataFrame(data=dic)\n",
    "df_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_overview(data_words,num_topics,df_topic):\n",
    "    topic_of_text=[]\n",
    "    for text in tqdm(data_words):\n",
    "        liste_text_top=[]\n",
    "        for t in range(num_topics):\n",
    "            liste_top=[]\n",
    "            for element in df_topic[f'{t}_word'].tolist():\n",
    "                liste_top.append(len([i for i, e in enumerate(text) if e == element]))\n",
    "    \n",
    "            Result = []\n",
    "            for i1, i2 in zip(liste_top, df_topic[f'{t}_per'].tolist()):\n",
    "                Result.append(i1*i2)\n",
    "            liste_text_top.append(sum(Result))    \n",
    "        if sum(liste_text_top)!=0:\n",
    "            topic_of_text.append([max(range(len(liste_text_top)), key=liste_text_top.__getitem__),max(liste_text_top)/sum(liste_text_top),liste_text_top])\n",
    "        else:\n",
    "            topic_of_text.append(['None','None',liste_text_top])\n",
    "    return pd.DataFrame(data=topic_of_text,columns=['topic_num','percent','perc_list'])\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=dic).to_csv(f'LDA/lda_gesamt_{num_topics}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m overview\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mmerge(df[[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtopic\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m]], create_df_overview(data_words,num_topics,df_topic), left_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, right_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m overview\u001b[39m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# overview=pd.merge(df[['text','topic','gender','age']], create_df_overview(data_words,num_topics,df_topic), left_index=True, right_index=True)\n",
    "# overview.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ml --> prediction welches richtige topic man hat --> meist mehr topics --> ggf noch pca --> (downstream task) --> klassifikation\n",
    "- einrichtigen text --> grundlagen teil und praxis teil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build App\n",
    "import socket\n",
    "import plotly.express as px\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import Dash, dcc, html, Input, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vis_topic(overview):\n",
    "    sock = socket.socket()\n",
    "    sock.bind(('', 0))\n",
    "    sock.getsockname()[1]\n",
    "    app = JupyterDash(__name__)\n",
    "    app.layout = html.Div([\n",
    "        html.H1(\"Übersicht der Verteilungen basierend auf den initialen Topics\"),\n",
    "        dcc.Graph(id='graph'),\n",
    "        html.Label([\n",
    "            \"Topics\",\n",
    "            dcc.Dropdown(\n",
    "                id='colorscale-dropdown', clearable=False,\n",
    "                value='plasma', options=[\n",
    "                    {'label': c, 'value': c}\n",
    "                    for c in overview['topic'].unique().tolist()\n",
    "                ]\n",
    "                )\n",
    "        ]),\n",
    "    ])\n",
    "    # Define callback to update graph\n",
    "    @app.callback(\n",
    "        Output('graph', 'figure'),\n",
    "        [Input(\"colorscale-dropdown\", \"value\")]\n",
    "    )\n",
    "    def update_figure(colorscale):\n",
    "        testdf = overview[overview['topic']==colorscale].topic_num.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "        return px.pie(testdf, values='counts', names='unique_values')\n",
    "    # Run app and display result inline in the notebook\n",
    "    app.run_server(mode='inline', port=sock.getsockname()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_topic_num(overview):\n",
    "    sock = socket.socket()\n",
    "    sock.bind(('', 0))\n",
    "    sock.getsockname()[1]\n",
    "    app = JupyterDash(__name__)\n",
    "    app.layout = html.Div([\n",
    "        html.H1(\"Übersicht der Verteilungen basierend auf den initialen Topics\"),\n",
    "        dcc.Graph(id='graph'),\n",
    "        html.Label([\n",
    "            \"Topics\",\n",
    "            dcc.Dropdown(\n",
    "                id='colorscale-dropdown', clearable=False,\n",
    "                value='plasma', options=[\n",
    "                    {'label': c, 'value': c}\n",
    "                    for c in overview['topic_num'].unique().tolist()\n",
    "                ]\n",
    "                )\n",
    "        ]),\n",
    "    ])\n",
    "    # Define callback to update graph\n",
    "    @app.callback(\n",
    "        Output('graph', 'figure'),\n",
    "        [Input(\"colorscale-dropdown\", \"value\")]\n",
    "    )\n",
    "    def update_figure(colorscale):\n",
    "        testdf = overview[overview['topic_num']==colorscale].topic.value_counts().rename_axis('topic').reset_index(name='counts')\n",
    "        return px.pie(testdf, values='counts', names='topic')\n",
    "    # Run app and display result inline in the notebook\n",
    "    app.run_server(mode='inline', port=sock.getsockname()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:57860/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x62a0ef220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#vis_topic(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:64260/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x3cfdf61f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#vis_topic_num(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.121*\"sleep\" + 0.096*\"water\" + 0.076*\"drink\" + 0.060*\"type\" + 0.058*\"wake\" + 0.045*\"kinda\" + 0.035*\"pm\" + 0.034*\"am\" + 0.022*\"pizza\" + 0.022*\"evil\" + 0.021*\"milk\" + 0.020*\"chair\" + 0.020*\"board\" + 0.019*\"wash\" + 0.017*\"dish\" + 0.013*\"nasty\" + 0.012*\"perform\" + 0.011*\"repeat\" + 0.011*\"randomly\" + 0.010*\"pig\" + 0.010*\"cream\" + 0.010*\"firework\" + 0.009*\"btw\" + 0.009*\"watch_tv\" + 0.009*\"ppl\" + 0.008*\"lip\" + 0.007*\"tonite\" + 0.007*\"hall\" + 0.007*\"plenty\" + 0.007*\"afterwards\"\n",
      "Topic: 1 \n",
      "Words: 0.119*\"part\" + 0.066*\"learn\" + 0.049*\"experience\" + 0.046*\"group\" + 0.046*\"student\" + 0.044*\"busy\" + 0.037*\"apparently\" + 0.036*\"lead\" + 0.031*\"personal\" + 0.030*\"member\" + 0.030*\"touch\" + 0.026*\"build\" + 0.019*\"treat\" + 0.017*\"lack\" + 0.017*\"certainly\" + 0.015*\"got\" + 0.014*\"horrible\" + 0.013*\"rise\" + 0.013*\"station\" + 0.013*\"language\" + 0.012*\"pocket\" + 0.012*\"freedom\" + 0.012*\"button\" + 0.011*\"ignore\" + 0.011*\"concern\" + 0.010*\"education\" + 0.008*\"holy\" + 0.008*\"proper\" + 0.008*\"activity\" + 0.007*\"minor\"\n",
      "Topic: 2 \n",
      "Words: 0.038*\"get\" + 0.021*\"so\" + 0.020*\"time\" + 0.018*\"good\" + 0.017*\"day\" + 0.016*\"now\" + 0.016*\"work\" + 0.015*\"see\" + 0.014*\"more\" + 0.013*\"take\" + 0.012*\"here\" + 0.012*\"come\" + 0.012*\"back\" + 0.012*\"well\" + 0.010*\"today\" + 0.010*\"look\" + 0.009*\"too\" + 0.009*\"there\" + 0.009*\"very\" + 0.009*\"new\" + 0.009*\"little\" + 0.009*\"year\" + 0.009*\"start\" + 0.009*\"do\" + 0.008*\"much\" + 0.008*\"still\" + 0.008*\"first\" + 0.008*\"week\" + 0.007*\"leave\" + 0.007*\"last\"\n",
      "Topic: 3 \n",
      "Words: 0.162*\"movie\" + 0.113*\"add\" + 0.064*\"dog\" + 0.055*\"create\" + 0.054*\"film\" + 0.050*\"rule\" + 0.049*\"character\" + 0.031*\"air\" + 0.027*\"difference\" + 0.024*\"neighbor\" + 0.014*\"bird\" + 0.014*\"ship\" + 0.014*\"grandmother\" + 0.014*\"introduce\" + 0.013*\"bore\" + 0.013*\"rid\" + 0.012*\"murder\" + 0.011*\"female\" + 0.009*\"script\" + 0.009*\"performance\" + 0.009*\"launch\" + 0.009*\"establish\" + 0.009*\"married\" + 0.008*\"squeeze\" + 0.008*\"awful\" + 0.007*\"cinema\" + 0.007*\"intellectual\" + 0.004*\"curiosity\" + 0.004*\"spanish\" + 0.003*\"backyard\"\n",
      "Topic: 4 \n",
      "Words: 0.389*\"go\" + 0.153*\"then\" + 0.050*\"eat\" + 0.030*\"mom\" + 0.030*\"home\" + 0.024*\"bed\" + 0.019*\"lol\" + 0.018*\"sister\" + 0.015*\"dinner\" + 0.015*\"lunch\" + 0.014*\"walk\" + 0.012*\"brother\" + 0.011*\"shop\" + 0.009*\"join\" + 0.009*\"boring\" + 0.009*\"attend\" + 0.008*\"land\" + 0.008*\"shower\" + 0.008*\"pound\" + 0.007*\"yell\" + 0.007*\"police\" + 0.007*\"breakfast\" + 0.007*\"dry\" + 0.007*\"doctor\" + 0.006*\"gym\" + 0.005*\"haha\" + 0.005*\"bite\" + 0.004*\"normally\" + 0.004*\"settle\" + 0.004*\"fall_asleep\"\n",
      "Topic: 5 \n",
      "Words: 0.074*\"top\" + 0.061*\"country\" + 0.049*\"piece\" + 0.037*\"program\" + 0.035*\"page\" + 0.033*\"beer\" + 0.033*\"shoe\" + 0.027*\"history\" + 0.026*\"public\" + 0.026*\"size\" + 0.026*\"simply\" + 0.025*\"smart\" + 0.022*\"article\" + 0.021*\"travel\" + 0.020*\"main\" + 0.020*\"cheer\" + 0.020*\"teacher\" + 0.020*\"wat\" + 0.019*\"ton\" + 0.018*\"appreciate\" + 0.017*\"park\" + 0.016*\"american\" + 0.016*\"angry\" + 0.015*\"remove\" + 0.013*\"idiot\" + 0.012*\"potato\" + 0.011*\"national\" + 0.009*\"emotional\" + 0.009*\"info\" + 0.009*\"garden\"\n",
      "Topic: 6 \n",
      "Words: 0.369*\"word\" + 0.059*\"fair\" + 0.058*\"restaurant\" + 0.019*\"powerful\" + 0.017*\"sentence\" + 0.012*\"meter\" + 0.000*\"meaning\" + 0.000*\"picture\" + 0.000*\"horse\" + 0.000*\"sometime\" + 0.000*\"lift\" + 0.000*\"stress\" + 0.000*\"publish\" + 0.000*\"kid\" + 0.000*\"order\" + 0.000*\"prey\" + 0.000*\"effect\" + 0.000*\"toll\" + 0.000*\"snake\" + 0.000*\"town\" + 0.000*\"gravity\" + 0.000*\"dream\" + 0.000*\"intrinsic\" + 0.000*\"vivid\" + 0.000*\"gratefulness\" + 0.000*\"distill\" + 0.000*\"wall\" + 0.000*\"blackout\" + 0.000*\"woman\" + 0.000*\"bear\"\n",
      "Topic: 7 \n",
      "Words: 0.732*\"urllink\" + 0.016*\"medium\" + 0.013*\"dig\" + 0.013*\"mystery\" + 0.012*\"cancel\" + 0.011*\"quiz\" + 0.007*\"pose\" + 0.006*\"relatively\" + 0.006*\"supposedly\" + 0.006*\"planning\" + 0.005*\"rose\" + 0.005*\"era\" + 0.004*\"romance\" + 0.003*\"requirement\" + 0.003*\"economy\" + 0.003*\"detective\" + 0.000*\"picture\" + 0.000*\"kid\" + 0.000*\"design\" + 0.000*\"woman\" + 0.000*\"company\" + 0.000*\"full\" + 0.000*\"area\" + 0.000*\"clean\" + 0.000*\"dream\" + 0.000*\"bear\" + 0.000*\"town\" + 0.000*\"support\" + 0.000*\"date\" + 0.000*\"sell\"\n",
      "Topic: 8 \n",
      "Words: 0.148*\"short\" + 0.096*\"large\" + 0.064*\"sun\" + 0.052*\"accept\" + 0.043*\"st\" + 0.034*\"interview\" + 0.031*\"personally\" + 0.025*\"swim\" + 0.025*\"apply\" + 0.022*\"wander\" + 0.021*\"necessary\" + 0.021*\"trash\" + 0.020*\"library\" + 0.015*\"favor\" + 0.013*\"industry\" + 0.013*\"will\" + 0.011*\"soap\" + 0.010*\"nicely\" + 0.008*\"dictionary\" + 0.006*\"pastor\" + 0.006*\"fren\" + 0.006*\"lecture\" + 0.005*\"minister\" + 0.004*\"seperate\" + 0.003*\"webpage\" + 0.002*\"exclusively\" + 0.002*\"spoiled\" + 0.000*\"stitch\" + 0.000*\"woman\" + 0.000*\"row\"\n",
      "Topic: 9 \n",
      "Words: 0.272*\"change\" + 0.113*\"throw\" + 0.078*\"white\" + 0.073*\"color\" + 0.060*\"involve\" + 0.028*\"baseball\" + 0.016*\"generation\" + 0.014*\"terribly\" + 0.014*\"combination\" + 0.010*\"justice\" + 0.008*\"jack\" + 0.007*\"craft\" + 0.005*\"bunny\" + 0.005*\"impressive\" + 0.000*\"woman\" + 0.000*\"kid\" + 0.000*\"cheese\" + 0.000*\"baby\" + 0.000*\"clean\" + 0.000*\"cap\" + 0.000*\"dream\" + 0.000*\"picture\" + 0.000*\"town\" + 0.000*\"folk\" + 0.000*\"toss\" + 0.000*\"full\" + 0.000*\"meat\" + 0.000*\"fish\" + 0.000*\"shitty\" + 0.000*\"area\"\n",
      "Topic: 10 \n",
      "Words: 0.094*\"wanna\" + 0.051*\"research\" + 0.045*\"pink\" + 0.043*\"coz\" + 0.042*\"and\" + 0.038*\"dunno\" + 0.032*\"goin\" + 0.030*\"it\" + 0.027*\"cent\" + 0.025*\"promote\" + 0.023*\"midnight\" + 0.021*\"whore\" + 0.000*\"woman\" + 0.000*\"boss\" + 0.000*\"picture\" + 0.000*\"tea\" + 0.000*\"sell\" + 0.000*\"paint\" + 0.000*\"design\" + 0.000*\"ratio\" + 0.000*\"square\" + 0.000*\"wit\" + 0.000*\"kid\" + 0.000*\"full\" + 0.000*\"weed\" + 0.000*\"fat\" + 0.000*\"bout\" + 0.000*\"dream\" + 0.000*\"arrange\" + 0.000*\"painting\"\n",
      "Topic: 11 \n",
      "Words: 0.199*\"name\" + 0.091*\"less\" + 0.052*\"fill\" + 0.046*\"shirt\" + 0.044*\"record\" + 0.043*\"report\" + 0.038*\"floor\" + 0.033*\"currently\" + 0.032*\"card\" + 0.026*\"mess\" + 0.023*\"inch\" + 0.022*\"rate\" + 0.021*\"buck\" + 0.020*\"thus\" + 0.016*\"pot\" + 0.015*\"english\" + 0.015*\"flat\" + 0.013*\"access\" + 0.012*\"science\" + 0.010*\"appeal\" + 0.009*\"needless_say\" + 0.008*\"heck\" + 0.006*\"hockey\" + 0.006*\"saturday\" + 0.005*\"horribly\" + 0.004*\"flesh\" + 0.004*\"disc\" + 0.003*\"confident\" + 0.003*\"thingie\" + 0.002*\"preach\"\n",
      "Topic: 12 \n",
      "Words: 0.090*\"high\" + 0.068*\"link\" + 0.056*\"continue\" + 0.054*\"study\" + 0.052*\"complete\" + 0.043*\"hair\" + 0.039*\"quick\" + 0.036*\"bored\" + 0.036*\"result\" + 0.035*\"cook\" + 0.030*\"proud\" + 0.026*\"speed\" + 0.025*\"leg\" + 0.022*\"click\" + 0.020*\"ahead\" + 0.019*\"exam\" + 0.018*\"available\" + 0.015*\"average\" + 0.014*\"math\" + 0.013*\"slowly\" + 0.011*\"toast\" + 0.011*\"aim\" + 0.011*\"bastard\" + 0.009*\"guilty\" + 0.009*\"senior\" + 0.009*\"south\" + 0.008*\"favourite\" + 0.007*\"celebrity\" + 0.005*\"junior\" + 0.005*\"consist\"\n",
      "Topic: 13 \n",
      "Words: 0.041*\"people\" + 0.026*\"other\" + 0.019*\"use\" + 0.017*\"most\" + 0.016*\"show\" + 0.015*\"man\" + 0.012*\"many\" + 0.012*\"all\" + 0.012*\"own\" + 0.012*\"run\" + 0.011*\"story\" + 0.011*\"place\" + 0.011*\"world\" + 0.011*\"same\" + 0.011*\"old\" + 0.010*\"point\" + 0.010*\"side\" + 0.009*\"song\" + 0.009*\"head\" + 0.009*\"hand\" + 0.009*\"make\" + 0.009*\"believe\" + 0.009*\"turn\" + 0.009*\"mind\" + 0.009*\"hear\" + 0.008*\"fact\" + 0.008*\"sound\" + 0.008*\"bring\" + 0.008*\"up\" + 0.007*\"kind\"\n",
      "Topic: 14 \n",
      "Words: 0.128*\"cold\" + 0.094*\"store\" + 0.081*\"dress\" + 0.053*\"near\" + 0.040*\"gain\" + 0.033*\"winter\" + 0.032*\"tie\" + 0.030*\"buddy\" + 0.022*\"quarter\" + 0.022*\"knock\" + 0.021*\"laptop\" + 0.012*\"tender\" + 0.011*\"turkey\" + 0.010*\"comic\" + 0.006*\"analysis\" + 0.003*\"lace\" + 0.000*\"horse\" + 0.000*\"heat\" + 0.000*\"woman\" + 0.000*\"cheese\" + 0.000*\"kid\" + 0.000*\"dream\" + 0.000*\"fish\" + 0.000*\"smoke\" + 0.000*\"salt\" + 0.000*\"clean\" + 0.000*\"sauce\" + 0.000*\"loss\" + 0.000*\"picture\" + 0.000*\"sell\"\n",
      "Topic: 15 \n",
      "Words: 0.117*\"easy\" + 0.088*\"lie\" + 0.071*\"test\" + 0.061*\"sex\" + 0.058*\"information\" + 0.047*\"ground\" + 0.033*\"alive\" + 0.032*\"correct\" + 0.027*\"tank\" + 0.022*\"personality\" + 0.019*\"mainly\" + 0.017*\"alright\" + 0.014*\"hrs\" + 0.012*\"useless\" + 0.011*\"painful\" + 0.011*\"annoy\" + 0.011*\"sword\" + 0.010*\"accompany\" + 0.009*\"unfair\" + 0.008*\"disgusting\" + 0.008*\"after\" + 0.006*\"embrace\" + 0.006*\"hatred\" + 0.006*\"instal\" + 0.005*\"believer\" + 0.003*\"brownie\" + 0.003*\"goodnight\" + 0.003*\"introduction\" + 0.001*\"internet_connection\" + 0.000*\"woman\"\n",
      "Topic: 16 \n",
      "Words: 0.036*\"number\" + 0.032*\"taste\" + 0.031*\"power\" + 0.031*\"issue\" + 0.028*\"choose\" + 0.027*\"act\" + 0.025*\"force\" + 0.025*\"form\" + 0.025*\"simple\" + 0.024*\"title\" + 0.021*\"receive\" + 0.021*\"government\" + 0.019*\"mouth\" + 0.018*\"human\" + 0.017*\"poem\" + 0.016*\"exist\" + 0.016*\"random\" + 0.015*\"remain\" + 0.015*\"stare\" + 0.015*\"original\" + 0.014*\"television\" + 0.014*\"fool\" + 0.013*\"faith\" + 0.012*\"suffer\" + 0.012*\"charge\" + 0.011*\"judge\" + 0.011*\"ear\" + 0.011*\"tough\" + 0.011*\"politic\" + 0.011*\"society\"\n",
      "Topic: 17 \n",
      "Words: 0.099*\"lovely\" + 0.061*\"exchange\" + 0.049*\"bloody\" + 0.000*\"trade\" + 0.000*\"hemisphere\" + 0.000*\"area\" + 0.000*\"full\" + 0.000*\"election\" + 0.000*\"internship\" + 0.000*\"educational\" + 0.000*\"kid\" + 0.000*\"holiday\" + 0.000*\"particularly\" + 0.000*\"support\" + 0.000*\"table\" + 0.000*\"woman\" + 0.000*\"wool\" + 0.000*\"stress\" + 0.000*\"poster\" + 0.000*\"chew\" + 0.000*\"quiet\" + 0.000*\"immediately\" + 0.000*\"picture\" + 0.000*\"labor\" + 0.000*\"develop\" + 0.000*\"direct\" + 0.000*\"discourse\" + 0.000*\"provide\" + 0.000*\"important\" + 0.000*\"elite\"\n",
      "Topic: 18 \n",
      "Words: 0.045*\"car\" + 0.040*\"drive\" + 0.031*\"free\" + 0.027*\"moment\" + 0.027*\"hot\" + 0.021*\"note\" + 0.020*\"light\" + 0.019*\"ride\" + 0.019*\"excited\" + 0.018*\"usually\" + 0.018*\"beat\" + 0.017*\"weather\" + 0.016*\"road\" + 0.016*\"arm\" + 0.016*\"red\" + 0.015*\"evening\" + 0.015*\"practice\" + 0.014*\"action\" + 0.013*\"low\" + 0.013*\"definitely\" + 0.012*\"black\" + 0.012*\"memory\" + 0.012*\"of_course\" + 0.012*\"forever\" + 0.011*\"drunk\" + 0.011*\"base\" + 0.010*\"relate\" + 0.010*\"brain\" + 0.010*\"egg\" + 0.010*\"local\"\n",
      "Topic: 19 \n",
      "Words: 0.292*\"read\" + 0.191*\"book\" + 0.078*\"follow\" + 0.036*\"invite\" + 0.028*\"shut\" + 0.028*\"topic\" + 0.026*\"grant\" + 0.022*\"pic\" + 0.020*\"likely\" + 0.014*\"constant\" + 0.011*\"indicate\" + 0.011*\"profile\" + 0.000*\"kid\" + 0.000*\"woman\" + 0.000*\"picture\" + 0.000*\"author\" + 0.000*\"dream\" + 0.000*\"order\" + 0.000*\"example\" + 0.000*\"writing\" + 0.000*\"worth\" + 0.000*\"political\" + 0.000*\"fiction\" + 0.000*\"town\" + 0.000*\"reader\" + 0.000*\"important\" + 0.000*\"captain\" + 0.000*\"conversation\" + 0.000*\"full\" + 0.000*\"offer\"\n",
      "Topic: 20 \n",
      "Words: 0.047*\"just\" + 0.040*\"know\" + 0.038*\"think\" + 0.032*\"say\" + 0.031*\"thing\" + 0.030*\"really\" + 0.028*\"so\" + 0.026*\"want\" + 0.025*\"make\" + 0.021*\"love\" + 0.020*\"feel\" + 0.019*\"even\" + 0.019*\"need\" + 0.019*\"tell\" + 0.019*\"friend\" + 0.018*\"way\" + 0.018*\"life\" + 0.015*\"like\" + 0.014*\"talk\" + 0.013*\"give\" + 0.013*\"never\" + 0.012*\"let\" + 0.012*\"mean\" + 0.012*\"ever\" + 0.012*\"maybe\" + 0.012*\"one\" + 0.011*\"find\" + 0.011*\"always\" + 0.011*\"ask\" + 0.011*\"right\"\n",
      "Topic: 21 \n",
      "Words: 0.097*\"state\" + 0.097*\"foot\" + 0.094*\"team\" + 0.054*\"death\" + 0.047*\"war\" + 0.035*\"soul\" + 0.030*\"peace\" + 0.027*\"shake\" + 0.026*\"hole\" + 0.021*\"attack\" + 0.021*\"knee\" + 0.020*\"sink\" + 0.019*\"lap\" + 0.019*\"above\" + 0.018*\"plastic\" + 0.017*\"flash\" + 0.016*\"climb\" + 0.014*\"hero\" + 0.013*\"wound\" + 0.013*\"slight\" + 0.011*\"confidence\" + 0.010*\"suspect\" + 0.008*\"stroke\" + 0.006*\"ending\" + 0.005*\"innocent\" + 0.004*\"forest\" + 0.004*\"primarily\" + 0.004*\"strangely\" + 0.002*\"acting\" + 0.000*\"sway\"\n",
      "Topic: 22 \n",
      "Words: 0.165*\"check\" + 0.127*\"site\" + 0.052*\"wedding\" + 0.051*\"vote\" + 0.048*\"marry\" + 0.046*\"interested\" + 0.042*\"goal\" + 0.038*\"photo\" + 0.020*\"sadly\" + 0.018*\"pleasure\" + 0.018*\"response\" + 0.016*\"speech\" + 0.015*\"accomplish\" + 0.015*\"discussion\" + 0.011*\"borrow\" + 0.010*\"amazingly\" + 0.010*\"candidate\" + 0.009*\"gorgeous\" + 0.009*\"halfway\" + 0.008*\"ceremony\" + 0.006*\"attendance\" + 0.005*\"harsh\" + 0.004*\"supporter\" + 0.003*\"bash\" + 0.002*\"thoughtful\" + 0.000*\"picture\" + 0.000*\"kid\" + 0.000*\"woman\" + 0.000*\"electoral_college\" + 0.000*\"support\"\n",
      "Topic: 23 \n",
      "Words: 0.477*\"nbsp\" + 0.071*\"comment\" + 0.054*\"email\" + 0.050*\"website\" + 0.021*\"art\" + 0.019*\"otherwise\" + 0.013*\"previous\" + 0.010*\"massive\" + 0.010*\"specifically\" + 0.009*\"successful\" + 0.008*\"pilot\" + 0.007*\"fortunately\" + 0.007*\"west\" + 0.007*\"function\" + 0.007*\"dragon\" + 0.007*\"posting\" + 0.007*\"classroom\" + 0.007*\"mum\" + 0.006*\"fake\" + 0.005*\"we\" + 0.005*\"hr\" + 0.005*\"best\" + 0.004*\"spit\" + 0.004*\"alternative\" + 0.003*\"lad\" + 0.003*\"arrival\" + 0.002*\"sausage\" + 0.002*\"await\" + 0.001*\"ethic\" + 0.001*\"spaghetti\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(num_topics,num_words=30):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el65948111942146567329523995\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el65948111942146567329523995_data = {\"mdsDat\": {\"x\": [-0.4621901620080707, 0.251755053735072, 0.36763162717846337, 0.14841860534013546, -0.2122609851133854, -0.08765299710549528, 0.11562250682495671, -0.12132364885167607], \"y\": [0.10675768523642672, 0.30189603060046777, -0.09582656762988198, 0.018304127691385633, -0.3037028760785243, 0.32816587624359894, -0.34089804201184887, -0.014696234051623852], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [46.887151259547096, 10.015108379819997, 8.537643951342188, 7.701462378869614, 7.506452543073336, 7.24317373925472, 7.08325255887795, 5.025755189215096]}, \"tinfo\": {\"Term\": [\"die\", \"und\", \"fur\", \"der\", \"ist\", \"auch\", \"werden\", \"nicht\", \"muss\", \"dem\", \"al\", \"wollen\", \"mussen\", \"den\", \"oder\", \"wird\", \"zu\", \"eine\", \"wir\", \"nur\", \"fordern\", \"haben\", \"mit\", \"sowie\", \"ihre\", \"sollen\", \"sind\", \"starken\", \"un\", \"afd\", \"die\", \"und\", \"der\", \"den\", \"zu\", \"eine\", \"mit\", \"sind\", \"durch\", \"einer\", \"afd\", \"bei\", \"nach\", \"unsere\", \"hat\", \"daher\", \"diese\", \"insbesondere\", \"lehnen\", \"wieder\", \"bleiben\", \"keine\", \"gegen\", \"fuhren\", \"seine\", \"gild\", \"schutz\", \"einem\", \"vor_allem\", \"sozialer\", \"afd_fordert\", \"auch\", \"muss\", \"al\", \"un\", \"unserer\", \"sich\", \"so\", \"ab\", \"kapitel\", \"besser\", \"verbessern\", \"machen\", \"fuhrt\", \"aber\", \"regionaler\", \"bekampfen\", \"hinaus\", \"sie\", \"staatliche\", \"dazu_gehort\", \"unserem\", \"selbst\", \"tragen\", \"anderen\", \"begrenzen\", \"sichern\", \"betrachten\", \"zwischen\", \"entgegen\", \"verbraucher\", \"wollen\", \"oder\", \"wird\", \"wir\", \"fordern\", \"mehr\", \"damit\", \"wie\", \"europaischen\", \"beim\", \"aufgrund\", \"zum\", \"vom\", \"jedoch\", \"verhindert\", \"bis\", \"rahman\", \"bringen\", \"vermitteln\", \"ring\", \"internet\", \"statt\", \"entsprechende\", \"zunehmend\", \"innere\", \"dort\", \"gefahrdet\", \"guter\", \"legen\", \"erleichtern\", \"afd_fordert\", \"starken\", \"ihrer\", \"unter\", \"ohne\", \"deutsche\", \"dabei\", \"immer\", \"weiter\", \"einen\", \"neue\", \"aller\", \"lehnt\", \"setzen\", \"digitalen\", \"geben\", \"dieser\", \"verhindern\", \"schutzen\", \"digitale\", \"gesetzliche\", \"europaischer\", \"entlasten\", \"neuen\", \"hier\", \"internationalen\", \"des\", \"gro\\u00dfen\", \"liegt\", \"stark\", \"grundsatzlich\", \"ist\", \"nicht\", \"nur\", \"sowie\", \"zur\", \"sondern\", \"dafur\", \"fordert\", \"schaffen\", \"bzw\", \"mindesten\", \"sorgen\", \"durfen\", \"dass\", \"gescheitert\", \"integration\", \"digitaler\", \"diesis\", \"gehoren\", \"stellt\", \"gute\", \"befurworten\", \"belastet\", \"zeigen\", \"gerecht\", \"einsetzen\", \"derzeitige\", \"alternative\", \"richtet\", \"sicherstellen\", \"migration\", \"werden\", \"mussen\", \"haben\", \"sollen\", \"soll\", \"darf\", \"erhalten\", \"setzt\", \"ein\", \"bereit\", \"konnen\", \"sozialen\", \"deshalb\", \"man\", \"entscheiden\", \"unterstutzt\", \"erhohen\", \"sehr\", \"ausgestattet\", \"wurde\", \"sein\", \"geleistet\", \"verbleiben\", \"anspruch\", \"schafft\", \"globale\", \"nationale\", \"kommen\", \"sollten\", \"schulen\", \"afd_fordert\", \"fur\", \"es\", \"unseren\", \"uber\", \"dazu\", \"soziale\", \"in\", \"beenden\", \"hohe\", \"besonder\", \"denen\", \"sollte\", \"ausbauen\", \"leisten\", \"reformieren\", \"politische\", \"einfuhren\", \"deutlich\", \"schon\", \"deren\", \"erhalt\", \"nationalen\", \"gefuhrt\", \"ebenso\", \"nehman\", \"erreichen\", \"verpflichtende\", \"kulturelle\", \"staatlichen\", \"gibt\", \"dem\", \"ihre\", \"vor\", \"unterstutzen\", \"noch\", \"alle\", \"ermoglichen\", \"stehen\", \"kann\", \"auf\", \"starker\", \"indem\", \"nutzen\", \"konsequent\", \"diesem\", \"jede\", \"vermeiden\", \"vielen\", \"gilt\", \"landlichen\", \"regionale\", \"notwendigen\", \"kulturellen\", \"darum\", \"tragt\", \"stattdessen\", \"hohen\", \"wichtige\", \"junge\", \"verstarken\"], \"Freq\": [11516.0, 8030.0, 2246.0, 7082.0, 1577.0, 1390.0, 1217.0, 961.0, 911.0, 633.0, 781.0, 729.0, 667.0, 2267.0, 669.0, 656.0, 2112.0, 2056.0, 590.0, 519.0, 501.0, 416.0, 1439.0, 370.0, 303.0, 334.0, 1182.0, 327.0, 339.0, 1060.0, 11515.412099858906, 8029.671932320718, 7081.360075546488, 2266.527312920567, 2111.5641382790623, 2056.0682997122826, 1438.969347551763, 1181.5001342499804, 1044.7407890640106, 982.1918617400333, 1059.505224318349, 663.7738481714762, 518.8190497029229, 496.4207071090785, 395.60685099339065, 276.95671507034825, 273.5545713449781, 258.3573234096107, 254.7207208069784, 253.59439191423442, 168.65861336980421, 156.26676699508437, 147.17785009377508, 146.295765798213, 123.26915757547721, 114.62165632270923, 111.39365110181376, 102.76715926681234, 96.43469665151412, 85.08951145736742, 238.68314596104975, 1389.7488230118506, 910.3119162052822, 780.8311952441939, 338.76283677529807, 314.43433927187425, 278.29610282452774, 251.90809342667353, 201.686604631294, 191.45657083681598, 150.7005694343469, 141.44456269210383, 118.18537067426836, 118.16234088659279, 101.8624103102233, 94.21759517078273, 91.40822700724665, 71.23527651484726, 70.77030350608025, 68.30048908498841, 63.50169829768902, 61.56941933184402, 60.022120763532634, 59.933128983535504, 59.13992565448623, 58.43460782464044, 55.05720170416711, 53.624360648737536, 50.801493732055825, 50.54059761777185, 50.22330245764222, 729.1107474996624, 668.199060259284, 655.6123743568116, 589.2536844076205, 500.6657680611486, 309.1613475473378, 282.5454286442409, 257.68301877725077, 143.65103087744305, 119.78961702832878, 119.313135778188, 114.04236176126287, 98.54071147049933, 95.49338966804788, 90.51253968822431, 72.6142734956203, 68.68571755312352, 67.05556134688965, 66.86576683617604, 65.04921664199576, 65.06612857522467, 64.302888041171, 61.80461917874286, 60.55118195272068, 59.67214681739815, 58.27286343690573, 57.23540474690464, 55.2970659633245, 52.85955764170078, 50.859401888243205, 71.56059657903481, 326.63202544835485, 296.0913412796332, 228.62414712258234, 226.8337077217506, 215.84112538090525, 210.61516453147146, 203.6414635149133, 194.85157267842175, 180.56295305241846, 177.6437160549037, 174.2053970703001, 157.34405863419826, 134.5221525704392, 119.73537521784966, 118.88789358791587, 107.3909367738142, 101.44184558770709, 90.0385162504282, 84.18954740542807, 73.60298406157509, 72.73573972117441, 65.6585421424548, 65.46086186824222, 60.07211223528144, 59.81282481956171, 59.174434465951954, 58.12966519843736, 56.74352291936899, 56.61758471972167, 55.80658557682521, 1576.4135509941868, 960.311244708914, 518.7938109445556, 369.3619252285046, 282.50389520221466, 239.2920593877393, 173.84065904365227, 147.29292573890393, 118.24071922782322, 105.72387834978386, 93.47324144620778, 92.05626676695822, 74.03013814763032, 69.67311202559765, 67.33309386078356, 59.8503336620685, 56.355060089852444, 54.301708102636226, 53.48922748555743, 52.35473724786673, 51.01150702522857, 47.73463249079361, 41.11763516263419, 41.12070070602216, 38.084118530937566, 37.143092384484675, 35.16053092975699, 33.51926531008987, 33.477818685116155, 33.13437417920237, 63.7581919021243, 1216.4373851341393, 666.2976269793274, 415.8831857256229, 333.6986596764831, 267.8165130038211, 258.90539206510624, 241.89918002773004, 165.92658035548104, 164.14385765037594, 151.4648378616096, 137.0296368013699, 92.05160018874693, 91.46779635321732, 71.19028722879294, 68.9502056166136, 59.772889484910884, 56.76390254042046, 47.702837888024696, 45.65575865736302, 45.302830675168174, 42.22227417790211, 41.7878293751966, 42.2678715915645, 38.72860603603511, 36.565833080831204, 35.89530414926378, 35.43373707865432, 35.24311957957893, 32.19521289226447, 31.431679315325358, 50.20326788462375, 2245.6811229588716, 229.15946959785938, 159.8746959174517, 156.22272951898256, 139.59486954074566, 123.57156791578672, 108.96554553349517, 104.48425429474575, 97.91665067113647, 82.42753433346874, 82.22967121229587, 79.58394787302339, 75.69722402842957, 71.85939419237198, 68.82404601726596, 67.412845611173, 66.13261103068373, 57.54524248374822, 57.57346995259387, 57.29637506485703, 55.23663878172885, 48.773541493336765, 48.846995355829975, 48.01928050444442, 46.125783098008114, 45.44968180363535, 44.941410566567704, 43.7989692178595, 43.27376676580328, 42.10183403173395, 632.7429757665113, 302.5120175311319, 252.93847266404399, 200.26774092096707, 161.47599521937545, 149.6411174988322, 115.07689928730808, 100.44164825512513, 95.70195135508376, 88.28121189714395, 86.8648618093255, 81.5439325920455, 73.94684197725711, 70.57039818415902, 67.36256012524439, 66.93968994047015, 51.287893552909466, 45.45373800374816, 45.27476055609929, 42.01689752922733, 40.86898959192975, 39.720374371032484, 39.484416555197036, 39.18077036437542, 38.694618314890704, 37.80623274777394, 34.62264784384134, 34.02642378328432, 33.968119593718725, 32.451674197075455], \"Total\": [11516.0, 8030.0, 2246.0, 7082.0, 1577.0, 1390.0, 1217.0, 961.0, 911.0, 633.0, 781.0, 729.0, 667.0, 2267.0, 669.0, 656.0, 2112.0, 2056.0, 590.0, 519.0, 501.0, 416.0, 1439.0, 370.0, 303.0, 334.0, 1182.0, 327.0, 339.0, 1060.0, 11516.309115552807, 8030.568748473524, 7082.257036251039, 2267.424295620568, 2112.4611736565826, 2056.9652247252084, 1439.8661482792124, 1182.3971718498933, 1045.6379047042972, 983.0888712001583, 1060.6429971900284, 664.6707677796813, 519.7161366404349, 497.3180343636727, 396.503684894793, 277.8537446229374, 274.4515138718981, 259.2541983081169, 255.61897157422666, 254.49165362399154, 169.55627451976306, 157.1639488866324, 148.07512753100434, 147.1933720806948, 124.16667821507244, 115.51848078240836, 112.29149122302458, 103.66401716223852, 97.3310636336114, 85.98699065672987, 531.5320304678444, 1390.6394498448287, 911.202839448302, 781.7220323235123, 339.6530367605153, 315.32511826868193, 279.18703278749115, 252.79845372731324, 202.58236565130917, 192.35369974154875, 151.59140401638197, 142.33500319646424, 119.07560550235472, 119.05553190388727, 102.75309468029303, 95.11475790831076, 92.29900398337597, 72.12621034822553, 71.66052429982481, 69.19165197763373, 64.39277814339516, 62.459726133488594, 60.91335316054269, 60.8242451363761, 60.0304967580388, 59.32588302526713, 55.9479232534602, 54.51965231824408, 51.69183613815478, 51.43191043669902, 51.13428513846435, 729.9916725166222, 669.0802426914894, 656.4938187649952, 590.1347048082, 501.54714459539116, 310.04236029474185, 283.4265908470236, 258.5642025139844, 144.5321841611229, 120.67090132971107, 120.19625866550712, 114.92331638753998, 99.4220702966276, 96.37666179730715, 91.39805042075055, 73.49525456248973, 69.56716754742246, 67.93706791140845, 67.7489369929936, 65.93054580515496, 65.952753066994, 65.18538573262738, 62.686369540080904, 61.43396100848658, 60.57086792503953, 59.153852231903414, 58.11928607693004, 56.17960826594912, 53.74240105467217, 51.74123654011765, 531.5320304678444, 327.51621395300566, 296.9760335567295, 229.50936937633833, 227.7182911529446, 216.72651451485376, 211.49934348869843, 204.52638172757472, 195.73563849669523, 181.44751933533706, 178.5277498324697, 175.09019735963568, 158.23764024987747, 135.40653295788124, 120.61984845956238, 119.77303827795112, 108.27558223807637, 102.32698031309248, 90.92282226959777, 85.07399861780627, 74.48859836800499, 73.6204657332007, 66.54421962418128, 66.3447713605294, 60.956919321548675, 60.69713993288279, 60.060173197081994, 59.013817350727145, 57.629048072932896, 57.50230513890993, 56.6916676277154, 1577.321004028102, 961.2185196587243, 519.7013533527266, 370.26976850304504, 283.4113416234792, 240.19967922680135, 174.74723302549575, 148.20166600800962, 119.14740937554724, 106.63184298372629, 94.3812117645153, 92.96282241945237, 74.93789014389547, 70.57995674082963, 68.27938729212205, 60.75889283424679, 57.26344438823836, 55.20937080545657, 54.39677202496036, 53.26339217622615, 51.91791194777547, 48.64905861073322, 42.02804317163847, 42.031220583944965, 38.99272980933006, 38.04961792210432, 36.08872576499429, 34.42881766506829, 34.391443092831636, 34.04487353116372, 65.76104327689572, 1217.330714859479, 667.1912339348971, 416.7763757681723, 334.59239213146594, 268.71020748679655, 259.79923277770166, 242.79308083541244, 166.82164453731434, 165.03758753829715, 152.35826488516577, 137.92267322586784, 92.94504221918658, 92.36134581514133, 72.08339482520603, 69.84413690058668, 60.668716988771735, 57.657881928918776, 48.59702680285656, 46.55286754955859, 46.19687465030079, 43.11531967090118, 42.68187528026474, 43.17520051289528, 39.622736400698194, 37.46003412993259, 36.789385370644595, 36.328820868164065, 36.139371234704385, 33.08883060427953, 32.3330559424504, 531.5320304678444, 2246.5725966167793, 230.05099174831147, 160.76692783787232, 157.11554174620875, 140.48619246257203, 124.46317328305524, 109.85754955857682, 105.37663206160079, 98.80890872262592, 83.31874489708773, 83.12183242112289, 80.47613824460792, 76.58864359658088, 72.75158517077098, 69.71745933985922, 68.30618440988091, 67.02404381812417, 58.436823943860205, 58.465541653062886, 58.1894364558475, 56.128663148432516, 49.66567862843122, 49.74918784896968, 48.91058238977247, 47.017145531709055, 46.341774508303125, 45.84540765506625, 44.691295213807486, 44.16627990455021, 42.99320290120952, 633.6366437912998, 303.4055852857407, 253.83203206287675, 201.16086802747054, 162.3696896709861, 150.53425157556873, 115.97108279690174, 101.33534731224994, 96.59549657101473, 89.17484989654119, 87.75834003256126, 82.43797312805822, 74.83982240858226, 71.46370504278482, 68.25615384293722, 67.83354671721325, 52.18291871395899, 46.34702180343022, 46.168346872903754, 42.91103683604052, 41.76649228852551, 40.6152380024398, 40.37942697645589, 40.07429696834041, 39.58983711731084, 38.70150426921158, 35.52183233208499, 34.920677271351934, 34.86251041471418, 33.347670790438016], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.5355, -1.896, -2.0217, -3.1609, -3.2317, -3.2584, -3.6152, -3.8124, -3.9354, -3.9971, -3.9214, -4.389, -4.6354, -4.6795, -4.9065, -5.2631, -5.2754, -5.3326, -5.3468, -5.3512, -5.759, -5.8354, -5.8953, -5.9013, -6.0726, -6.1453, -6.1739, -6.2545, -6.3181, -6.4432, -5.4118, -2.1064, -2.5295, -2.6829, -3.518, -3.5925, -3.7146, -3.8142, -4.0366, -4.0886, -4.328, -4.3914, -4.571, -4.5712, -4.7197, -4.7977, -4.8279, -5.0773, -5.0838, -5.1194, -5.1922, -5.2231, -5.2486, -5.25, -5.2634, -5.2754, -5.3349, -5.3613, -5.4153, -5.4205, -5.4268, -2.5918, -2.6791, -2.6981, -2.8048, -2.9677, -3.4498, -3.5398, -3.6319, -4.2163, -4.3979, -4.4019, -4.4471, -4.5932, -4.6246, -4.6782, -4.8985, -4.9541, -4.9781, -4.981, -5.0085, -5.0083, -5.0201, -5.0597, -5.0802, -5.0948, -5.1185, -5.1365, -5.1709, -5.216, -5.2546, -4.9131, -3.2918, -3.3899, -3.6485, -3.6564, -3.706, -3.7306, -3.7642, -3.8084, -3.8845, -3.9008, -3.9204, -4.0222, -4.1789, -4.2953, -4.3024, -4.4041, -4.4611, -4.5804, -4.6475, -4.7819, -4.7938, -4.8961, -4.8991, -4.985, -4.9894, -5.0001, -5.0179, -5.042, -5.0443, -5.0587, -1.692, -2.1877, -2.8034, -3.1432, -3.4112, -3.5773, -3.8968, -4.0625, -4.2822, -4.3941, -4.5173, -4.5325, -4.7505, -4.8111, -4.8453, -4.9631, -5.0233, -5.0604, -5.0755, -5.0969, -5.1229, -5.1893, -5.3385, -5.3384, -5.4151, -5.4402, -5.495, -5.5428, -5.5441, -5.5544, -4.8998, -1.9156, -2.5175, -2.9888, -3.209, -3.4289, -3.4628, -3.5307, -3.9077, -3.9185, -3.9989, -4.099, -4.4969, -4.5033, -4.7539, -4.7859, -4.9287, -4.9803, -5.1542, -5.1981, -5.2059, -5.2763, -5.2866, -5.2752, -5.3627, -5.4201, -5.4386, -5.4516, -5.457, -5.5474, -5.5714, -5.1032, -1.2801, -3.5625, -3.9225, -3.9456, -4.0582, -4.1801, -4.3059, -4.3479, -4.4128, -4.585, -4.5874, -4.6201, -4.6702, -4.7222, -4.7654, -4.7861, -4.8052, -4.9443, -4.9438, -4.9487, -4.9853, -5.1097, -5.1082, -5.1253, -5.1655, -5.1803, -5.1916, -5.2173, -5.2294, -5.2568, -2.2037, -2.9416, -3.1206, -3.3541, -3.5694, -3.6455, -3.9082, -4.0442, -4.0925, -4.1732, -4.1894, -4.2526, -4.3504, -4.3971, -4.4437, -4.45, -4.7163, -4.8371, -4.841, -4.9157, -4.9434, -4.9719, -4.9778, -4.9856, -4.9981, -5.0213, -5.1092, -5.1266, -5.1283, -5.174], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7573, 0.7573, 0.7573, 0.757, 0.757, 0.757, 0.7568, 0.7567, 0.7566, 0.7565, 0.7564, 0.7561, 0.7557, 0.7556, 0.7552, 0.7542, 0.7542, 0.754, 0.7539, 0.7539, 0.7521, 0.7517, 0.7513, 0.7513, 0.7502, 0.7496, 0.7494, 0.7487, 0.7482, 0.7469, -0.0432, 2.3004, 2.3001, 2.2999, 2.2985, 2.2982, 2.2979, 2.2975, 2.2966, 2.2964, 2.2952, 2.2948, 2.2936, 2.2935, 2.2924, 2.2916, 2.2914, 2.2886, 2.2886, 2.2881, 2.2871, 2.2867, 2.2863, 2.2863, 2.2861, 2.2859, 2.285, 2.2845, 2.2837, 2.2836, 2.2831, 2.4595, 2.4594, 2.4593, 2.4592, 2.4589, 2.4578, 2.4576, 2.4573, 2.4546, 2.4534, 2.4533, 2.453, 2.4518, 2.4515, 2.4509, 2.4486, 2.4479, 2.4476, 2.4476, 2.4472, 2.4472, 2.4471, 2.4465, 2.4462, 2.4457, 2.4457, 2.4454, 2.4449, 2.4441, 2.4435, 0.4555, 2.5611, 2.5608, 2.5599, 2.5599, 2.5597, 2.5596, 2.5594, 2.5592, 2.5589, 2.5588, 2.5587, 2.5581, 2.5572, 2.5564, 2.5563, 2.5556, 2.5551, 2.554, 2.5533, 2.5518, 2.5517, 2.5504, 2.5503, 2.5491, 2.5491, 2.5489, 2.5487, 2.5483, 2.5483, 2.548, 2.5888, 2.5885, 2.5877, 2.587, 2.5862, 2.5856, 2.5842, 2.5833, 2.5818, 2.5809, 2.5797, 2.5796, 2.5772, 2.5765, 2.5755, 2.5743, 2.5734, 2.5728, 2.5726, 2.5722, 2.5718, 2.5704, 2.5675, 2.5675, 2.5658, 2.5653, 2.5634, 2.5626, 2.5625, 2.5623, 2.5585, 2.6244, 2.6238, 2.623, 2.6224, 2.6218, 2.6217, 2.6214, 2.6197, 2.6197, 2.6192, 2.6186, 2.6155, 2.6154, 2.6126, 2.6122, 2.6102, 2.6095, 2.6065, 2.6057, 2.6056, 2.6042, 2.6039, 2.6039, 2.6023, 2.601, 2.6005, 2.6002, 2.6, 2.5977, 2.5968, 0.2654, 2.647, 2.6436, 2.6419, 2.6417, 2.6411, 2.6402, 2.6393, 2.6389, 2.6384, 2.6367, 2.6366, 2.6363, 2.6357, 2.6351, 2.6345, 2.6343, 2.634, 2.6321, 2.6321, 2.632, 2.6314, 2.6293, 2.6291, 2.629, 2.6283, 2.628, 2.6275, 2.6273, 2.627, 2.6265, 2.9892, 2.9876, 2.9871, 2.9861, 2.9851, 2.9846, 2.9829, 2.9817, 2.9813, 2.9805, 2.9804, 2.9797, 2.9786, 2.978, 2.9774, 2.9773, 2.9733, 2.9711, 2.971, 2.9695, 2.9689, 2.9683, 2.9682, 2.968, 2.9677, 2.9672, 2.965, 2.9647, 2.9646, 2.9634]}, \"token.table\": {\"Topic\": [2, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 2, 8, 4, 5, 2, 6, 2, 8, 3, 7, 6, 7, 5, 2, 1, 3, 2, 5, 6, 7, 2, 2, 3, 1, 3, 5, 4, 5, 1, 3, 6, 8, 5, 7, 2, 8, 1, 7, 1, 7, 5, 4, 6, 7, 4, 1, 1, 8, 4, 5, 4, 4, 5, 3, 1, 5, 7, 6, 1, 1, 4, 1, 7, 5, 2, 4, 6, 3, 7, 6, 6, 3, 8, 7, 7, 3, 4, 3, 5, 1, 2, 7, 4, 3, 7, 1, 5, 6, 5, 5, 4, 7, 1, 8, 6, 4, 4, 5, 3, 6, 1, 4, 2, 7, 8, 8, 4, 4, 7, 8, 3, 1, 5, 4, 3, 5, 8, 3, 8, 8, 2, 1, 6, 6, 8, 7, 8, 8, 3, 1, 4, 7, 4, 2, 6, 3, 5, 5, 1, 2, 6, 1, 6, 7, 7, 4, 4, 5, 8, 8, 5, 8, 3, 4, 7, 3, 7, 8, 2, 5, 3, 5, 6, 7, 6, 1, 4, 6, 6, 1, 2, 4, 6, 2, 2, 5, 2, 1, 2, 6, 6, 7, 6, 5, 5, 5, 7, 6, 1, 2, 7, 4, 4, 8, 3, 8, 8, 5, 2, 8, 7, 2, 1, 1, 2, 7, 2, 4, 8, 6, 2, 6, 2, 4, 3, 8, 3, 7, 8, 8, 3, 8, 1, 4, 6, 8, 3, 1, 3, 3, 3, 6, 5, 1, 3, 3, 5, 2], \"Freq\": [0.9971252895115681, 0.9926708321278671, 0.9993937666191811, 0.4496436457265553, 0.08089822914745555, 0.13545749996783255, 0.06961010415013617, 0.05832197915281679, 0.09406770831099483, 0.07337281248257596, 0.039508437490617825, 0.9990763566924599, 0.9964509633523468, 0.9937735100189737, 0.9875448042032716, 0.9828337792673554, 0.9842833570503419, 0.9995401756760891, 0.9868253224098025, 0.9900474550640035, 0.9923142182843521, 0.9881238776758483, 0.9869360783821973, 0.9866583520983072, 0.977650850562099, 0.9989908270196356, 0.9944402393425573, 0.9859261321648722, 0.9755391140282206, 0.9910850593750886, 0.9841722904165612, 0.9960986968870738, 0.9904685320586649, 0.9932614076182478, 0.9967192336506648, 0.986206824341751, 0.9940745375298191, 0.9976390305498745, 0.9957239207021565, 0.9969273596650785, 0.9984948806470532, 0.9969236522788906, 0.9731923689344038, 0.9917829824838341, 0.9965392153203841, 0.9939002764794448, 0.9989952541451983, 0.9998128733023689, 0.9865037573349044, 0.9998225090893194, 0.9795592374098689, 0.9698319699042869, 0.9823481495199301, 0.9852606541933026, 0.9925248513800158, 0.9966477820377443, 0.999886325076926, 0.9983548501317109, 0.9815964748639701, 0.9882191144881436, 0.9780948272401423, 0.9873757124943522, 0.994861140455087, 0.9779362837542154, 0.9804940474987172, 0.9993899372799826, 0.9874844335476414, 0.9813827121804446, 0.9937130228708877, 0.9995307530173062, 0.9935945260427318, 0.9975336155766892, 0.9988923979997566, 0.984721246887117, 0.9724144950876219, 0.9916022867314913, 0.99182168447906, 0.9879139905216642, 0.9890507371041476, 0.9798915013270891, 0.9967335113806226, 0.9885899046772162, 0.9856741626276572, 0.9916265091824452, 0.9710461128746221, 0.995431483514484, 0.9963178847381866, 0.991572102580154, 0.9989090864113431, 0.9918916835392074, 0.9918924876587476, 0.991134121304507, 0.9997451243651589, 0.9935458072278573, 0.9807415721616317, 0.984940701921726, 0.9927393104505061, 0.9743225200142493, 0.9840242427075357, 0.9745406434947131, 0.9812624667141724, 0.9934406287846751, 0.9768986064264223, 0.9955117070541728, 0.9746937685224017, 0.9785431215365051, 0.9828206783387373, 0.987799483475112, 0.9823199371211461, 0.9790029104445698, 0.9981371886380525, 0.9987296842022372, 0.9843017112380481, 0.9843855604947469, 0.9918134029301279, 0.9853095322559234, 0.9986632240624089, 0.9967134265178236, 0.9974263382399448, 0.9921939861027068, 0.9946872404616514, 0.9905752064549246, 0.9951622835182545, 0.9875097652566336, 0.9885144516915678, 0.9855540061227435, 0.9991625014662656, 0.9877118806615233, 0.9857158177961958, 0.9752596584567775, 0.9938351518222494, 0.9929624449991468, 0.9925940465680715, 0.9684728539601637, 0.9933102135835431, 0.993511320991443, 0.9845317704376152, 0.9658383716722827, 0.9787691721474473, 0.9861859343813663, 0.9975785382031125, 0.9921785976590457, 0.9896691574622495, 0.9890845312569314, 0.9909670373052737, 0.9849702580208225, 0.9966380068396108, 0.9732205696694833, 0.9853656067908783, 0.9993984522240157, 0.9986799432615571, 0.9982145539774683, 0.9986220619489243, 0.9634224057811756, 0.9865968079604543, 0.9783664975785679, 0.9970438778679228, 0.979730560028285, 0.9987323177468981, 0.9915643758772863, 0.9848520399559682, 0.9986504684888695, 0.9887784019048399, 0.9983854811088967, 0.9968457028668717, 0.9808775088058942, 0.9918471950574123, 0.9897090435214837, 0.9816481527050313, 0.9882798638946716, 0.9595410088179271, 0.9858859684264557, 0.9903698336240727, 0.9877193350028212, 0.992037332762169, 0.958771111990679, 0.9884987614915585, 0.989850488067105, 0.9877147463099231, 0.9741317081859904, 0.9906039347122454, 0.9850056988631792, 0.9969976857910712, 0.9950747126393988, 0.9957482524326454, 0.9830570430797612, 0.9693089319245887, 0.990782591862415, 0.9996640960758796, 0.9968415403039825, 0.9973569761512262, 0.9982295110546531, 0.9940834854281813, 0.9670937115517553, 0.9950054919695851, 0.98964293042752, 0.9965706935562724, 0.9962786318969878, 0.989832247136346, 0.9885216281068603, 0.9827775180448224, 0.9735934312993825, 0.9912646086500967, 0.9984238522216193, 0.9913587696362546, 0.9818151611852155, 0.9818739792559006, 0.9868224923714402, 0.9762802907474215, 0.9864487403908091, 0.9851012997208586, 0.9928998637957109, 0.998077341610887, 0.9999291770618822, 0.99734971532782, 0.992639638981028, 0.9952295671243669, 0.9957976127117383, 0.9977806162000162, 0.9942291558052334, 0.9889775650126984, 0.9906206964802499, 0.9727806588288043, 0.9778175223259137, 0.9870319605930686, 0.9956448696780935, 0.977331304129553, 0.9889454059910776, 0.9815596000055891, 0.9595872587651776, 0.9709361734364876, 0.9957547625454958, 0.9967221155812571, 0.9863243697960398, 0.9962416731958209, 0.9989068583884104, 0.9736351828403044, 0.9978179403471217, 0.9980680952911801, 0.9980772105098127, 0.9992477937325834, 0.9986415290010043, 0.9740918696478745, 0.9754653667055563, 0.9997816889312174, 0.9919658045332906, 0.992936138230992, 0.9985486056375765, 0.9866161430925817], \"Term\": [\"ab\", \"aber\", \"afd\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"al\", \"alle\", \"aller\", \"alternative\", \"anderen\", \"anspruch\", \"auch\", \"auf\", \"aufgrund\", \"ausbauen\", \"ausgestattet\", \"beenden\", \"befurworten\", \"begrenzen\", \"bei\", \"beim\", \"bekampfen\", \"belastet\", \"bereit\", \"besonder\", \"besser\", \"betrachten\", \"bis\", \"bleiben\", \"bringen\", \"bzw\", \"dabei\", \"dafur\", \"daher\", \"damit\", \"darf\", \"darum\", \"dass\", \"dazu\", \"dazu_gehort\", \"dem\", \"den\", \"denen\", \"der\", \"deren\", \"derzeitige\", \"des\", \"deshalb\", \"deutlich\", \"deutsche\", \"die\", \"diese\", \"diesem\", \"dieser\", \"diesis\", \"digitale\", \"digitalen\", \"digitaler\", \"dort\", \"durch\", \"durfen\", \"ebenso\", \"ein\", \"eine\", \"einem\", \"einen\", \"einer\", \"einfuhren\", \"einsetzen\", \"entgegen\", \"entlasten\", \"entscheiden\", \"entsprechende\", \"erhalt\", \"erhalten\", \"erhohen\", \"erleichtern\", \"ermoglichen\", \"erreichen\", \"es\", \"europaischen\", \"europaischer\", \"fordern\", \"fordert\", \"fuhren\", \"fuhrt\", \"fur\", \"geben\", \"gefahrdet\", \"gefuhrt\", \"gegen\", \"gehoren\", \"geleistet\", \"gerecht\", \"gescheitert\", \"gesetzliche\", \"gibt\", \"gild\", \"gilt\", \"globale\", \"gro\\u00dfen\", \"grundsatzlich\", \"gute\", \"guter\", \"haben\", \"hat\", \"hier\", \"hinaus\", \"hohe\", \"hohen\", \"ihre\", \"ihrer\", \"immer\", \"in\", \"indem\", \"innere\", \"insbesondere\", \"integration\", \"internationalen\", \"internet\", \"ist\", \"jede\", \"jedoch\", \"junge\", \"kann\", \"kapitel\", \"keine\", \"kommen\", \"konnen\", \"konsequent\", \"kulturelle\", \"kulturellen\", \"landlichen\", \"legen\", \"lehnen\", \"lehnt\", \"leisten\", \"liegt\", \"machen\", \"man\", \"mehr\", \"migration\", \"mindesten\", \"mit\", \"muss\", \"mussen\", \"nach\", \"nationale\", \"nationalen\", \"nehman\", \"neue\", \"neuen\", \"nicht\", \"noch\", \"notwendigen\", \"nur\", \"nutzen\", \"oder\", \"ohne\", \"politische\", \"rahman\", \"reformieren\", \"regionale\", \"regionaler\", \"richtet\", \"ring\", \"schaffen\", \"schafft\", \"schon\", \"schulen\", \"schutz\", \"schutzen\", \"sehr\", \"sein\", \"seine\", \"selbst\", \"setzen\", \"setzt\", \"sich\", \"sichern\", \"sicherstellen\", \"sie\", \"sind\", \"so\", \"soll\", \"sollen\", \"sollte\", \"sollten\", \"sondern\", \"sorgen\", \"sowie\", \"soziale\", \"sozialen\", \"sozialer\", \"staatliche\", \"staatlichen\", \"stark\", \"starken\", \"starker\", \"statt\", \"stattdessen\", \"stehen\", \"stellt\", \"tragen\", \"tragt\", \"uber\", \"un\", \"und\", \"unsere\", \"unserem\", \"unseren\", \"unserer\", \"unter\", \"unterstutzen\", \"unterstutzt\", \"verbessern\", \"verbleiben\", \"verbraucher\", \"verhindern\", \"verhindert\", \"vermeiden\", \"vermitteln\", \"verpflichtende\", \"verstarken\", \"vielen\", \"vom\", \"vor\", \"vor_allem\", \"weiter\", \"werden\", \"wichtige\", \"wie\", \"wieder\", \"wir\", \"wird\", \"wollen\", \"wurde\", \"zeigen\", \"zu\", \"zum\", \"zunehmend\", \"zur\", \"zwischen\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 3, 1, 4, 2, 5, 7, 8]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el65948111942146567329523995\", ldavis_el65948111942146567329523995_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el65948111942146567329523995\", ldavis_el65948111942146567329523995_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el65948111942146567329523995\", ldavis_el65948111942146567329523995_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "5     -0.462190  0.106758       1        1  46.887151\n",
       "2      0.251755  0.301896       2        1  10.015108\n",
       "0      0.367632 -0.095827       3        1   8.537644\n",
       "3      0.148419  0.018304       4        1   7.701462\n",
       "1     -0.212261 -0.303703       5        1   7.506453\n",
       "4     -0.087653  0.328166       6        1   7.243174\n",
       "6      0.115623 -0.340898       7        1   7.083253\n",
       "7     -0.121324 -0.014696       8        1   5.025755, topic_info=             Term          Freq         Total Category  logprob  loglift\n",
       "4             die  11516.000000  11516.000000  Default  30.0000  30.0000\n",
       "43            und   8030.000000   8030.000000  Default  29.0000  29.0000\n",
       "73            fur   2246.000000   2246.000000  Default  28.0000  28.0000\n",
       "86            der   7082.000000   7082.000000  Default  27.0000  27.0000\n",
       "1             ist   1577.000000   1577.000000  Default  26.0000  26.0000\n",
       "...           ...           ...           ...      ...      ...      ...\n",
       "1183  stattdessen     37.806233     38.701504   Topic8  -5.0213   2.9672\n",
       "608         hohen     34.622648     35.521832   Topic8  -5.1092   2.9650\n",
       "2018     wichtige     34.026424     34.920677   Topic8  -5.1266   2.9647\n",
       "927         junge     33.968120     34.862510   Topic8  -5.1283   2.9646\n",
       "922    verstarken     32.451674     33.347671   Topic8  -5.1740   2.9634\n",
       "\n",
       "[274 rows x 6 columns], token_table=       Topic      Freq         Term\n",
       "term                               \n",
       "1021       2  0.997125           ab\n",
       "99         2  0.992671         aber\n",
       "8285       1  0.999394          afd\n",
       "10548      1  0.449644  afd_fordert\n",
       "10548      2  0.080898  afd_fordert\n",
       "...      ...       ...          ...\n",
       "188        1  0.999782           zu\n",
       "241        3  0.991966          zum\n",
       "286        3  0.992936    zunehmend\n",
       "465        5  0.998549          zur\n",
       "57         2  0.986616     zwischen\n",
       "\n",
       "[249 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 3, 1, 4, 2, 5, 7, 8])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "pyLDAvis.enable_notebook()\n",
    "import os\n",
    "LDAvis_data_filepath = os.path.join('LDA/pic/ges_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word,mds='mmds',R=30)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'LDA/pic/ges_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nur Nomen und Verben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d29387779704b9cb0b03a9c87d5bbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemma_text_NV=lemmatization_test(text_liste,['NOUN','VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc679b55178045d5b46062ff8895c52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_words_NV=gen_words(lemma_text_NV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases_NV=gensim.models.Phrases(data_words_NV,min_count=3,threshold=100)\n",
    "trigram_phases_NV=gensim.models.Phrases(bigram_phrases_NV[data_words_NV],threshold=50)\n",
    "\n",
    "bigram_NV=gensim.models.phrases.Phraser(bigram_phrases_NV)\n",
    "trigram_NV=gensim.models.phrases.Phraser(trigram_phases_NV)\n",
    "\n",
    "def make_bigrams(bigram,texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "def make_trgram(trigram,bigram,texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams_NV=make_bigrams(bigram_NV,data_words_NV)\n",
    "data_bigrams_trigrams_NV=make_trgram(trigram_NV,bigram_NV,data_bigrams_NV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c840fb235cc443f87653c39a55ad7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18858 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word=corpora.Dictionary(data_bigrams_trigrams_NV)\n",
    "\n",
    "texts = data_bigrams_trigrams_NV\n",
    "\n",
    "corpus=[id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[0][0:90])\n",
    "\n",
    "tfidf=TfidfModel(corpus,id2word=id2word)\n",
    "\n",
    "low_value=0.03\n",
    "words=[]\n",
    "words_missing_in_tfdf=[]\n",
    "\n",
    "for i in tqdm(range(0,len(corpus))):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops=low_value_words+words_missing_in_tfdf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]  \n",
    "    corpus[i]=new_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=gensim.models.ldamodel.LdaModel(\n",
    "corpus=corpus,\n",
    "id2word=id2word,\n",
    "num_topics=num_topics,\n",
    "random_state=100,\n",
    "update_every=1,\n",
    "chunksize=100,\n",
    "passes=10,\n",
    "alpha='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.7440000000000004, 1: 0.6870000000000003, 2: 0.7470000000000003, 3: 0.7640000000000003, 4: 0.8040000000000003, 5: 0.7340000000000003, 6: 0.8540000000000002, 7: 0.8500000000000002}\n"
     ]
    }
   ],
   "source": [
    "dic={}\n",
    "dic_sum={}\n",
    "for idx, topic in lda_model.print_topics(num_topics,num_words=100):\n",
    "    elements=[]\n",
    "    percent=[]\n",
    "\n",
    "    for e in topic.split('+'):\n",
    "        elements.append(e.split('*')[1].replace('\"','').strip())\n",
    "        percent.append(float(e.split('*')[0].replace('\"','').strip()))\n",
    "\n",
    "    dic[str(idx)+'_word']=elements\n",
    "    dic[str(idx)+'_per']=percent\n",
    "    dic_sum[idx]=sum(percent)\n",
    "print(dic_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=dic).to_csv(f'LDA/lda_nv_{num_topics}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_word</th>\n",
       "      <th>0_per</th>\n",
       "      <th>1_word</th>\n",
       "      <th>1_per</th>\n",
       "      <th>2_word</th>\n",
       "      <th>2_per</th>\n",
       "      <th>3_word</th>\n",
       "      <th>3_per</th>\n",
       "      <th>4_word</th>\n",
       "      <th>4_per</th>\n",
       "      <th>5_word</th>\n",
       "      <th>5_per</th>\n",
       "      <th>6_word</th>\n",
       "      <th>6_per</th>\n",
       "      <th>7_word</th>\n",
       "      <th>7_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>al</td>\n",
       "      <td>0.088</td>\n",
       "      <td>durch</td>\n",
       "      <td>0.221</td>\n",
       "      <td>fur</td>\n",
       "      <td>0.250</td>\n",
       "      <td>oder</td>\n",
       "      <td>0.078</td>\n",
       "      <td>zu</td>\n",
       "      <td>0.188</td>\n",
       "      <td>werden</td>\n",
       "      <td>0.199</td>\n",
       "      <td>der</td>\n",
       "      <td>0.351</td>\n",
       "      <td>die</td>\n",
       "      <td>0.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mussen</td>\n",
       "      <td>0.077</td>\n",
       "      <td>sowie</td>\n",
       "      <td>0.059</td>\n",
       "      <td>wollen</td>\n",
       "      <td>0.063</td>\n",
       "      <td>auch</td>\n",
       "      <td>0.071</td>\n",
       "      <td>ist</td>\n",
       "      <td>0.119</td>\n",
       "      <td>dem</td>\n",
       "      <td>0.102</td>\n",
       "      <td>eine</td>\n",
       "      <td>0.114</td>\n",
       "      <td>und</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wird</td>\n",
       "      <td>0.066</td>\n",
       "      <td>es</td>\n",
       "      <td>0.047</td>\n",
       "      <td>ihrer</td>\n",
       "      <td>0.038</td>\n",
       "      <td>unsere</td>\n",
       "      <td>0.064</td>\n",
       "      <td>nicht</td>\n",
       "      <td>0.106</td>\n",
       "      <td>darf</td>\n",
       "      <td>0.043</td>\n",
       "      <td>mit</td>\n",
       "      <td>0.081</td>\n",
       "      <td>den</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mehr</td>\n",
       "      <td>0.032</td>\n",
       "      <td>gild</td>\n",
       "      <td>0.024</td>\n",
       "      <td>deutsche</td>\n",
       "      <td>0.028</td>\n",
       "      <td>hat</td>\n",
       "      <td>0.061</td>\n",
       "      <td>nur</td>\n",
       "      <td>0.050</td>\n",
       "      <td>unter</td>\n",
       "      <td>0.035</td>\n",
       "      <td>einer</td>\n",
       "      <td>0.055</td>\n",
       "      <td>afd</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>damit</td>\n",
       "      <td>0.030</td>\n",
       "      <td>mindesten</td>\n",
       "      <td>0.022</td>\n",
       "      <td>immer</td>\n",
       "      <td>0.025</td>\n",
       "      <td>unserer</td>\n",
       "      <td>0.045</td>\n",
       "      <td>haben</td>\n",
       "      <td>0.040</td>\n",
       "      <td>kapitel</td>\n",
       "      <td>0.031</td>\n",
       "      <td>bei</td>\n",
       "      <td>0.038</td>\n",
       "      <td>sind</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>sorgt</td>\n",
       "      <td>0.002</td>\n",
       "      <td>nichtstaatliche</td>\n",
       "      <td>0.000</td>\n",
       "      <td>festgelegten</td>\n",
       "      <td>0.001</td>\n",
       "      <td>einkomman</td>\n",
       "      <td>0.001</td>\n",
       "      <td>rund</td>\n",
       "      <td>0.001</td>\n",
       "      <td>familienpolitik</td>\n",
       "      <td>0.000</td>\n",
       "      <td>afd_bekennt</td>\n",
       "      <td>0.001</td>\n",
       "      <td>gestalten</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>nachhaltig</td>\n",
       "      <td>0.002</td>\n",
       "      <td>zustimman</td>\n",
       "      <td>0.000</td>\n",
       "      <td>konkreten</td>\n",
       "      <td>0.001</td>\n",
       "      <td>zuzulassen</td>\n",
       "      <td>0.001</td>\n",
       "      <td>fahren</td>\n",
       "      <td>0.001</td>\n",
       "      <td>eroffnet</td>\n",
       "      <td>0.000</td>\n",
       "      <td>weltweiten</td>\n",
       "      <td>0.001</td>\n",
       "      <td>bestimmte</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>gleich</td>\n",
       "      <td>0.002</td>\n",
       "      <td>heimischer</td>\n",
       "      <td>0.000</td>\n",
       "      <td>medizinischer</td>\n",
       "      <td>0.001</td>\n",
       "      <td>wirtschaftliche</td>\n",
       "      <td>0.001</td>\n",
       "      <td>weiterhin</td>\n",
       "      <td>0.001</td>\n",
       "      <td>grundlegenden</td>\n",
       "      <td>0.000</td>\n",
       "      <td>erweitern</td>\n",
       "      <td>0.001</td>\n",
       "      <td>abzulehnen</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>sichert</td>\n",
       "      <td>0.002</td>\n",
       "      <td>verbotene</td>\n",
       "      <td>0.000</td>\n",
       "      <td>familienpolitik</td>\n",
       "      <td>0.001</td>\n",
       "      <td>welcher</td>\n",
       "      <td>0.001</td>\n",
       "      <td>ideologisch</td>\n",
       "      <td>0.001</td>\n",
       "      <td>mobilitat</td>\n",
       "      <td>0.000</td>\n",
       "      <td>lokale</td>\n",
       "      <td>0.001</td>\n",
       "      <td>gibt</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>vertreten</td>\n",
       "      <td>0.002</td>\n",
       "      <td>steigen</td>\n",
       "      <td>0.000</td>\n",
       "      <td>werten</td>\n",
       "      <td>0.001</td>\n",
       "      <td>derart</td>\n",
       "      <td>0.001</td>\n",
       "      <td>kunftigen</td>\n",
       "      <td>0.001</td>\n",
       "      <td>heimischer</td>\n",
       "      <td>0.000</td>\n",
       "      <td>steigt</td>\n",
       "      <td>0.001</td>\n",
       "      <td>ermoglicht</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0_word  0_per           1_word  1_per           2_word  2_per  \\\n",
       "0           al  0.088            durch  0.221              fur  0.250   \n",
       "1       mussen  0.077            sowie  0.059           wollen  0.063   \n",
       "2         wird  0.066               es  0.047            ihrer  0.038   \n",
       "3         mehr  0.032             gild  0.024         deutsche  0.028   \n",
       "4        damit  0.030        mindesten  0.022            immer  0.025   \n",
       "..         ...    ...              ...    ...              ...    ...   \n",
       "95       sorgt  0.002  nichtstaatliche  0.000     festgelegten  0.001   \n",
       "96  nachhaltig  0.002        zustimman  0.000        konkreten  0.001   \n",
       "97      gleich  0.002       heimischer  0.000    medizinischer  0.001   \n",
       "98     sichert  0.002        verbotene  0.000  familienpolitik  0.001   \n",
       "99   vertreten  0.002          steigen  0.000           werten  0.001   \n",
       "\n",
       "             3_word  3_per       4_word  4_per           5_word  5_per  \\\n",
       "0              oder  0.078           zu  0.188           werden  0.199   \n",
       "1              auch  0.071          ist  0.119              dem  0.102   \n",
       "2            unsere  0.064        nicht  0.106             darf  0.043   \n",
       "3               hat  0.061          nur  0.050            unter  0.035   \n",
       "4           unserer  0.045        haben  0.040          kapitel  0.031   \n",
       "..              ...    ...          ...    ...              ...    ...   \n",
       "95        einkomman  0.001         rund  0.001  familienpolitik  0.000   \n",
       "96       zuzulassen  0.001       fahren  0.001         eroffnet  0.000   \n",
       "97  wirtschaftliche  0.001    weiterhin  0.001    grundlegenden  0.000   \n",
       "98          welcher  0.001  ideologisch  0.001        mobilitat  0.000   \n",
       "99           derart  0.001    kunftigen  0.001       heimischer  0.000   \n",
       "\n",
       "         6_word  6_per      7_word  7_per  \n",
       "0           der  0.351         die  0.332  \n",
       "1          eine  0.114         und  0.185  \n",
       "2           mit  0.081         den  0.066  \n",
       "3         einer  0.055         afd  0.035  \n",
       "4           bei  0.038        sind  0.034  \n",
       "..          ...    ...         ...    ...  \n",
       "95  afd_bekennt  0.001   gestalten  0.001  \n",
       "96   weltweiten  0.001   bestimmte  0.001  \n",
       "97    erweitern  0.001  abzulehnen  0.001  \n",
       "98       lokale  0.001        gibt  0.001  \n",
       "99       steigt  0.001  ermoglicht  0.001  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb74c0d1152c414ab9d03c0a9dbd16e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>percent</th>\n",
       "      <th>perc_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This blog is being posted due to the fact that...</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>0.522646</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So I have a big fucking interview tomorrow for...</td>\n",
       "      <td>Student</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>0.400248</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.055999999999999994, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was reminded just now of the time Ashley and...</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>0.318367</td>\n",
       "      <td>[0.084, 0.396, 0.0, 0.0, 0.14900000000000002, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was checking up on my cousin Dylan and Fanni...</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>0.316395</td>\n",
       "      <td>[0.0, 0.0, 0.025, 0.0, 0.018, 0.071, 0.015, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for the NME interview click urlLink part 1 and...</td>\n",
       "      <td>Student</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>0.93568</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.06, 0.026, 0.0, 0.0, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    topic  gender  age  \\\n",
       "0  This blog is being posted due to the fact that...  Student    male   14   \n",
       "1  So I have a big fucking interview tomorrow for...  Student    male   15   \n",
       "2  I was reminded just now of the time Ashley and...  Student  female   17   \n",
       "3  I was checking up on my cousin Dylan and Fanni...  Student  female   23   \n",
       "4  for the NME interview click urlLink part 1 and...  Student  female   23   \n",
       "\n",
       "  topic_num   percent                                          perc_list  \n",
       "0        18  0.522646  [0.0, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0...  \n",
       "1         8  0.400248  [0.0, 0.0, 0.0, 0.0, 0.055999999999999994, 0.1...  \n",
       "2         8  0.318367  [0.084, 0.396, 0.0, 0.0, 0.14900000000000002, ...  \n",
       "3        18  0.316395  [0.0, 0.0, 0.025, 0.0, 0.018, 0.071, 0.015, 0....  \n",
       "4        22   0.93568  [0.0, 0.0, 0.0, 0.0, 0.06, 0.026, 0.0, 0.0, 0....  "
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_nomen= pd.merge(df[['text','topic','gender','age']], create_df_overview(data_words,num_topics,pd.DataFrame(data=dic)), left_index=True, right_index=True)\n",
    "overview_nomen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:55936/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2dcb44f70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_topic(overview_nomen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:55940/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x502086f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_topic_num(overview_nomen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el65948117772208966856037143\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el65948117772208966856037143_data = {\"mdsDat\": {\"x\": [-0.28982127132572644, 0.12976053190244258, 0.37515180205958215, -0.33799190003024776, 0.25615556030894704, -0.020221923504584093, -0.20479496594783106, 0.09176216653741759], \"y\": [0.33440857218439896, 0.3942341865301766, -0.14963486041016733, -0.03110256364018957, 0.11359818522492458, -0.033884475922781716, -0.3047413063730846, -0.322877737593277], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [37.61712965321706, 18.597044475006072, 9.49340833157452, 9.193420539355039, 8.643080323847347, 6.2571618621264005, 5.80574462471115, 4.393010190162409]}, \"tinfo\": {\"Term\": [\"die\", \"der\", \"und\", \"fur\", \"zu\", \"eine\", \"werden\", \"durch\", \"ist\", \"mit\", \"den\", \"nicht\", \"al\", \"einer\", \"dem\", \"mussen\", \"wird\", \"oder\", \"wollen\", \"afd\", \"sind\", \"auch\", \"bei\", \"nur\", \"unsere\", \"hat\", \"haben\", \"ihrer\", \"sowie\", \"unserer\", \"die\", \"und\", \"den\", \"sind\", \"afd\", \"nach\", \"zur\", \"starken\", \"soll\", \"fordern\", \"erhalten\", \"sollen\", \"lehnt\", \"weiter\", \"wir\", \"besser\", \"verbessern\", \"ein\", \"konnen\", \"schaffen\", \"dieser\", \"muss\", \"regionaler\", \"schutzen\", \"besonder\", \"letzten\", \"entscheiden\", \"politische\", \"digitale\", \"bewahren\", \"afd_fordert\", \"der\", \"eine\", \"mit\", \"einer\", \"bei\", \"un\", \"bleiben\", \"unterstutzen\", \"aller\", \"fuhren\", \"vom\", \"starker\", \"verhindert\", \"konsequent\", \"etwa\", \"staatliche\", \"rahman\", \"bringen\", \"diesis\", \"internet\", \"bis\", \"guter\", \"bestehende\", \"internationalen\", \"beschleunigen\", \"pandemie\", \"ebenso\", \"verbraucher\", \"notwendigen\", \"gewahrleisten\", \"generelle\", \"umgehend\", \"afd_fordert\", \"zu\", \"ist\", \"nicht\", \"nur\", \"haben\", \"lehnen\", \"wie\", \"bereit\", \"uber\", \"bekampfen\", \"verhindern\", \"gescheitert\", \"gesetzliche\", \"leisten\", \"reformieren\", \"integration\", \"digitaler\", \"dazu_gehort\", \"ring\", \"migration\", \"daraus\", \"verfolgen\", \"begegnen\", \"schafft\", \"globale\", \"grundlegende\", \"orientiert\", \"spricht\", \"freiheitliche\", \"langer\", \"afd_fordert\", \"al\", \"mussen\", \"wird\", \"mehr\", \"damit\", \"vor\", \"dabei\", \"gegen\", \"setzt\", \"bzw\", \"seine\", \"sorgen\", \"vor_allem\", \"dafur\", \"sozialer\", \"ausbauen\", \"denen\", \"man\", \"dass\", \"jede\", \"gehoren\", \"statt\", \"begrenzen\", \"zunehmend\", \"besteht\", \"digitalen\", \"liegt\", \"gefahrdet\", \"anderen\", \"gewahrleistet\", \"afd_fordert\", \"fur\", \"wollen\", \"ihrer\", \"deutsche\", \"immer\", \"einen\", \"alle\", \"setzen\", \"dazu\", \"geben\", \"auf\", \"beenden\", \"nutzen\", \"europaischer\", \"halten\", \"indem\", \"sozialen\", \"entsprechend\", \"nehman\", \"verpflichtende\", \"stattdessen\", \"neuen\", \"hoch\", \"treten\", \"braucht\", \"hoheren\", \"offentlich\", \"ubernehman\", \"guten\", \"bundesweiten\", \"km\", \"afd_fordert\", \"oder\", \"auch\", \"unsere\", \"hat\", \"unserer\", \"wieder\", \"sich\", \"ab\", \"keine\", \"aufgrund\", \"aber\", \"kann\", \"ihre\", \"hinaus\", \"sie\", \"diesem\", \"einfuhren\", \"diese\", \"durfen\", \"nationaler\", \"entsprechende\", \"dort\", \"sichern\", \"weitere\", \"konsequente\", \"richtet\", \"beispielsweise\", \"unterstutzt\", \"vollstandige\", \"selbst\", \"werden\", \"dem\", \"darf\", \"unter\", \"kapitel\", \"ohne\", \"neue\", \"beim\", \"zum\", \"daher\", \"europaischen\", \"komman\", \"gute\", \"moglichst\", \"sehr\", \"zeigen\", \"fachliche\", \"seiner\", \"garantierten\", \"zerstort\", \"stellt\", \"strikt\", \"des\", \"machen\", \"rechtliche\", \"weil\", \"gezeigt\", \"entgegenwirken\", \"ersetzen\", \"begru\\u00dfen\", \"durch\", \"sowie\", \"es\", \"gild\", \"mindesten\", \"in\", \"wahrend\", \"offentliche\", \"staatlich\", \"gut\", \"fordert\", \"belastet\", \"ausgestattet\", \"gerecht\", \"zunehmende\", \"einsetzen\", \"nehmen\", \"auslandische\", \"moglich\", \"bedeutet\", \"gilt\", \"sehen\", \"entschieden\", \"gemeinsaman\", \"ausgebaute\", \"forderungen\", \"profitieren\", \"haufig\", \"ihnen\", \"befordert\", \"eingehalten\"], \"Freq\": [11791.0, 6180.0, 6562.0, 2044.0, 1686.0, 2002.0, 1094.0, 916.0, 1067.0, 1430.0, 2353.0, 950.0, 766.0, 964.0, 562.0, 668.0, 577.0, 460.0, 511.0, 1252.0, 1227.0, 418.0, 660.0, 452.0, 380.0, 360.0, 360.0, 314.0, 244.0, 269.0, 11791.032732552514, 6561.608963686251, 2352.706344975012, 1226.4249995820398, 1250.9577732160194, 520.9469992363083, 320.06378535134206, 302.74414103241827, 260.75232629864166, 249.60409441532798, 238.39499028605442, 222.69708791709758, 218.56486260240163, 179.95533241209412, 165.65464348848786, 148.92588718713358, 139.7793851286619, 139.82364832600973, 138.4900063040106, 108.84672114459154, 101.36882405460318, 98.795406802287, 93.12034493805908, 82.06174064757936, 82.05241772198757, 70.28944404789837, 69.68799115194955, 67.11295522116696, 66.80541459223159, 65.53092678150287, 203.54794778590755, 6179.196440148062, 2002.125792015975, 1429.2605095688677, 963.4514253669469, 659.8462372401, 318.9913594730049, 167.66322639079814, 166.03411488655073, 157.4727122413941, 145.43195089817428, 92.22773713343678, 84.140140543717, 79.93091618343773, 68.35580297640941, 66.05685385095586, 64.63993011280957, 60.65216071841637, 59.211573693186644, 58.92339407019161, 57.46463285457089, 53.628675255948316, 48.829092611134556, 47.724502977401606, 45.641613866722565, 43.52774267337021, 41.805263312632086, 40.39623143013305, 44.63590165260338, 38.47897304917961, 37.235154381221506, 37.34066701764244, 37.44213958065888, 83.00896910989414, 1685.4435200945668, 1066.8914401890402, 949.2394014495018, 451.68439348310164, 359.90983220163156, 230.98144125120177, 201.40594906854102, 133.72658937781432, 99.31492767788234, 78.91309367856462, 64.4463549972569, 66.68394543801823, 60.693494071511736, 60.607985985426396, 59.85623279323334, 59.24315807856612, 55.781307151088534, 54.817499254621865, 52.39848674098351, 64.51499215115288, 44.876512460638985, 37.05003812279793, 36.23330864780076, 32.28585737884696, 31.693477451447944, 30.879010946423662, 30.29881093774297, 29.941201085416495, 29.731752035396052, 28.4374272401287, 36.698364648765974, 765.186745891939, 667.7726556756448, 576.5973212010724, 279.3259003719039, 258.3507973122305, 253.69542985207468, 164.96875261418617, 151.50093546145033, 139.65111670096212, 118.78974298300695, 107.83499542916361, 103.43147399189317, 99.26675098667116, 97.39757028273456, 87.58953760744697, 74.72543054016673, 74.66019258598206, 71.3495121038116, 70.32436159670743, 67.1417409522706, 59.43839313910285, 58.791274722593606, 57.26776209438751, 55.36921621702884, 54.85656002308695, 54.389521832840444, 53.11641853560064, 52.335708005714466, 47.84866418020188, 42.73317893312807, 59.30516121046057, 2043.2338944193716, 510.7990378119673, 313.73728237690904, 228.70709144877216, 202.66667727926688, 191.2684525517939, 166.86250033659732, 142.53771115915194, 141.1502843499356, 125.97721741599324, 93.26474649461802, 79.22748269003836, 77.02267271345998, 77.07350424282227, 74.45737740148054, 63.35275421789219, 55.051938839746036, 54.476682683278966, 51.545423188422276, 50.26044321749999, 42.93051765968371, 40.689870693953885, 38.84311861896422, 37.799352517690856, 35.05634089545823, 34.23952463661978, 34.22568293723205, 33.582222739197, 32.173656710135084, 29.45979521980433, 43.796719640911554, 50.82167885108532, 459.8318048623516, 417.6063742240034, 379.7113863337999, 359.4401898326031, 268.8447141152525, 226.22235560583607, 171.84946233602764, 157.7113666568412, 140.34101810962048, 90.29882650665343, 87.09490044582829, 83.74993531106918, 67.6198458837714, 60.90922966305322, 60.218831221458885, 58.95115977312817, 56.960063977092894, 56.551611469563845, 53.63089839174565, 53.71343836693323, 49.30720956535101, 46.49006947515223, 43.691808414184436, 36.87604365803241, 35.70526564583509, 32.828778084989864, 31.79419957066014, 29.662767086517277, 29.492404092946177, 28.065516927276896, 1093.8328700985417, 561.1804714519326, 238.4084364258707, 193.71402088153803, 172.84164693653298, 129.857898372881, 108.5925339173383, 97.62541821675782, 91.77843461729707, 75.64357855360286, 71.42101604641555, 53.22157970461709, 51.656226797398716, 45.596171434869426, 44.04033097939797, 42.56298997460002, 42.324795320432, 40.99893538159106, 38.32844361834195, 34.911712538656126, 34.966318407550474, 34.53150582275527, 32.157101259878196, 30.213591337538396, 28.332548729975148, 25.02031405399069, 24.34379012142086, 23.502387951465398, 25.55158525077741, 19.3770188305515, 915.9080593144848, 243.19806673867427, 193.33332442893723, 100.83899783913897, 89.76219843897172, 85.39896193481596, 60.65241339745376, 58.12149348953557, 56.18795587710988, 50.82654501664966, 50.39760012343418, 39.49114875876383, 39.04842332675815, 36.57271453060814, 35.83017825790651, 35.66772028671835, 33.88920696724486, 32.173177929114345, 30.011137866022136, 28.994797023414698, 28.336964845513712, 22.88515860121839, 22.1699049332392, 21.76331715607107, 21.295560108278092, 19.49118362212202, 18.507449243806118, 18.250544495467462, 17.989130033901368, 17.79305806815087, 18.764725289288716], \"Total\": [11791.0, 6180.0, 6562.0, 2044.0, 1686.0, 2002.0, 1094.0, 916.0, 1067.0, 1430.0, 2353.0, 950.0, 766.0, 964.0, 562.0, 668.0, 577.0, 460.0, 511.0, 1252.0, 1227.0, 418.0, 660.0, 452.0, 380.0, 360.0, 360.0, 314.0, 244.0, 269.0, 11791.872233562013, 6562.448291110912, 2353.5458227769886, 1227.2645299375624, 1252.0832718113238, 521.7866083772567, 320.90323143456095, 303.5831444068928, 261.5919413283258, 250.44334633724884, 239.23474195639494, 223.53666795028238, 219.41156123464125, 180.79422588005738, 166.49353996844656, 149.76545472688002, 140.61849501264095, 140.66311158958072, 139.32895000939695, 109.68575854193938, 102.20818385718417, 99.63505551729023, 93.96532928274829, 82.90079907313437, 82.89156576778629, 71.12941101244824, 70.52763862401699, 67.95369824237711, 67.64456344041118, 66.37112040573896, 462.5030142760689, 6180.041438969501, 2002.9706980037035, 1430.1052565799864, 964.2964103908175, 660.6910927560534, 319.8357545021372, 168.5087955585862, 166.8783911344055, 158.31775703601824, 146.27763108093134, 93.07248852954821, 84.98475857322583, 80.78005139736383, 69.20041929171659, 66.90135090081236, 65.48531288510645, 61.497209416311485, 60.05683281061522, 59.76860158876545, 58.31420829848898, 54.47322836162619, 49.675394856756135, 48.569416471024425, 46.48618115367818, 44.37360453006035, 42.64950823373086, 41.240864256339314, 45.60812935013574, 39.32474928012368, 38.079664748848174, 38.192208980683766, 38.37686422212098, 462.5030142760689, 1686.2993878623654, 1067.7471436223793, 950.0951351520014, 452.54037936839217, 360.7650803583885, 231.83853718827328, 202.26155978985372, 134.58208468399837, 100.17191495297364, 79.76874606459201, 65.30256691589284, 67.57880301733248, 61.55007637142968, 61.464005957920875, 60.71317288723681, 60.0999492998018, 56.63813951085039, 55.67408164213448, 53.254041759535674, 65.74310808684338, 45.75233279986637, 37.90633608799128, 37.09254488877339, 33.14201457706976, 32.54966104665977, 31.749739349534725, 31.157597012357225, 30.80643903531128, 30.610735354450895, 29.29455641019888, 462.5030142760689, 766.0273926131108, 668.6132901494365, 577.4379107304044, 280.16610808055316, 259.19116219268324, 254.53582893236415, 165.80893041524013, 152.34186659580436, 140.49281506857037, 119.63103513729436, 108.67645387686305, 104.2714661602523, 100.10682107725239, 98.23747057845158, 88.43080014463672, 75.5656886920242, 75.5016340132455, 72.18956936555888, 71.16454423834386, 67.98240610817919, 60.27915882664314, 59.63281886194883, 58.10875152937315, 56.21116982687845, 55.69838160215303, 55.22976269004919, 53.9577824019557, 53.17906621228353, 48.6890367767087, 43.57671063444027, 462.5030142760689, 2044.0574920737051, 511.6223327214332, 314.5610747742196, 229.53147066637115, 203.4906640326948, 192.09208764369495, 167.685743207997, 143.36119125614132, 141.973737581765, 126.80131298949094, 94.08852689778172, 80.05266743690608, 77.84588406534061, 77.89729124860312, 75.28117360160662, 64.17652022556098, 55.87573963267406, 55.30189743155673, 52.36896898235882, 51.093815536041284, 43.755719170111995, 41.51296034036793, 39.6690650427959, 38.62276093513914, 35.87946345104034, 35.064139828953174, 35.050856261991214, 34.40617377919706, 32.997732371776706, 30.284809360061313, 45.60446166968348, 462.5030142760689, 460.6883806247662, 418.46322645107284, 380.56842947783133, 360.29683367770525, 269.70154342016264, 227.0793617456906, 172.70639111263364, 158.57598503803365, 141.19798874280542, 91.15780409141686, 87.9515284312891, 84.60646032787287, 68.47686765890059, 61.76611029665748, 61.07501313316971, 59.807650723927864, 57.816483875342236, 57.40863343753248, 54.488410192639286, 54.57399915711644, 50.16441925051053, 47.3462957084416, 44.54866557777534, 37.73277889264351, 36.56624047369714, 33.691569379403354, 32.65036711694853, 30.52663195896988, 30.356653966793886, 28.92273455872825, 1094.6832815313508, 562.031007324413, 239.25930159786012, 194.56506485095736, 173.69799712313124, 130.70840707702186, 109.44265383997406, 98.4758423985734, 92.62867806719062, 76.4947167459375, 72.2717138007428, 54.07302968423637, 52.50605898776606, 46.4464883100667, 44.89160385133152, 43.4168049649799, 43.18578785862767, 41.84931131808236, 39.202242742347494, 35.76822629163857, 35.829031260838676, 35.390996063459596, 33.010525806460315, 31.064299012379742, 29.18450473682893, 25.87090511095055, 25.194184794006635, 24.359179454802895, 26.492484666586968, 20.23598918861049, 916.7672904884965, 244.0573672715076, 194.19216971460912, 101.6979245397354, 90.62159064282682, 86.25819125283289, 61.51195962767795, 58.98015315900865, 57.04797905594206, 51.68504685439527, 51.256959066306926, 40.353008800535726, 39.91323731644009, 37.433183737071566, 36.6914810450566, 36.526018110084294, 34.748247908775795, 33.039026226881525, 30.87037371679478, 29.853651884775854, 29.19550365255935, 23.74454822884715, 23.029579347117966, 22.622157404075086, 22.1870594362371, 20.35626405961625, 19.365703587928333, 19.111760303501473, 18.849323792328956, 18.6596910377479, 19.816566892715752], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.1039, -1.69, -2.7157, -3.3671, -3.3473, -4.2234, -4.7105, -4.7661, -4.9154, -4.9591, -5.0051, -5.0732, -5.0919, -5.2863, -5.3691, -5.4756, -5.5389, -5.5386, -5.5482, -5.7891, -5.8602, -5.886, -5.9451, -6.0715, -6.0716, -6.2264, -6.235, -6.2726, -6.2772, -6.2965, -5.1631, -1.0456, -2.1726, -2.5096, -2.904, -3.2825, -4.0094, -4.6526, -4.6624, -4.7153, -4.7948, -5.2503, -5.3421, -5.3934, -5.5498, -5.584, -5.6057, -5.6694, -5.6934, -5.6983, -5.7234, -5.7925, -5.8862, -5.9091, -5.9537, -6.0011, -6.0415, -6.0758, -5.976, -6.1244, -6.1573, -6.1545, -6.1518, -5.3556, -1.6724, -2.1296, -2.2465, -2.9892, -3.2163, -3.6598, -3.7968, -4.2063, -4.5038, -4.7338, -4.9363, -4.9022, -4.9963, -4.9977, -5.0102, -5.0205, -5.0807, -5.0981, -5.1433, -4.9352, -5.2982, -5.4899, -5.5122, -5.6275, -5.646, -5.6721, -5.691, -5.7029, -5.7099, -5.7544, -5.4994, -2.4299, -2.5661, -2.7129, -3.4377, -3.5157, -3.5339, -3.9643, -4.0494, -4.1309, -4.2927, -4.3894, -4.4311, -4.4722, -4.4912, -4.5974, -4.7562, -4.7571, -4.8024, -4.8169, -4.8632, -4.9851, -4.996, -5.0223, -5.056, -5.0653, -5.0739, -5.0975, -5.1124, -5.202, -5.3151, -4.9873, -1.386, -2.7723, -3.2597, -3.5759, -3.6967, -3.7546, -3.8911, -4.0487, -4.0585, -4.1722, -4.4729, -4.636, -4.6642, -4.6635, -4.6981, -4.8596, -5.0, -5.0105, -5.0658, -5.0911, -5.2487, -5.3023, -5.3488, -5.376, -5.4513, -5.4749, -5.4753, -5.4943, -5.5372, -5.6253, -5.2287, -5.08, -2.5544, -2.6507, -2.7459, -2.8007, -3.0911, -3.2638, -3.5387, -3.6245, -3.7412, -4.1821, -4.2183, -4.2574, -4.4714, -4.5759, -4.5873, -4.6086, -4.6429, -4.6501, -4.7031, -4.7016, -4.7872, -4.846, -4.9081, -5.0777, -5.11, -5.194, -5.226, -5.2954, -5.3011, -5.3507, -1.6129, -2.2803, -3.1364, -3.344, -3.458, -3.744, -3.9228, -4.0293, -4.091, -4.2844, -4.3418, -4.6359, -4.6658, -4.7906, -4.8253, -4.8594, -4.865, -4.8968, -4.9642, -5.0576, -5.056, -5.0685, -5.1398, -5.2021, -5.2664, -5.3907, -5.4181, -5.4533, -5.3697, -5.6463, -1.5116, -2.8377, -3.0671, -3.718, -3.8344, -3.8842, -4.2264, -4.269, -4.3029, -4.4031, -4.4116, -4.6555, -4.6668, -4.7323, -4.7528, -4.7573, -4.8085, -4.8604, -4.93, -4.9644, -4.9874, -5.2011, -5.2328, -5.2513, -5.2731, -5.3616, -5.4134, -5.4274, -5.4418, -5.4528, -5.3996], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9776, 0.9776, 0.9774, 0.977, 0.9768, 0.9761, 0.9751, 0.9749, 0.9745, 0.9744, 0.9742, 0.9739, 0.9738, 0.9731, 0.9727, 0.9721, 0.9717, 0.9717, 0.9717, 0.97, 0.9695, 0.9692, 0.9687, 0.9675, 0.9675, 0.9658, 0.9657, 0.9653, 0.9652, 0.965, 0.157, 1.682, 1.6817, 1.6816, 1.6813, 1.6809, 1.6795, 1.6771, 1.6771, 1.6768, 1.6764, 1.673, 1.6722, 1.6716, 1.6699, 1.6695, 1.6692, 1.6683, 1.668, 1.6679, 1.6675, 1.6665, 1.665, 1.6646, 1.6638, 1.6629, 1.6622, 1.6615, 1.6606, 1.6604, 1.6597, 1.6596, 1.6575, -0.0355, 2.3541, 2.3538, 2.3537, 2.3527, 2.3522, 2.3509, 2.3503, 2.3482, 2.346, 2.3438, 2.3414, 2.3412, 2.3406, 2.3405, 2.3404, 2.3402, 2.3393, 2.3391, 2.3384, 2.3357, 2.3352, 2.3317, 2.3311, 2.3284, 2.3279, 2.3268, 2.3266, 2.3261, 2.3254, 2.3249, -0.1793, 2.3856, 2.3854, 2.3852, 2.3837, 2.3834, 2.3834, 2.3816, 2.3811, 2.3807, 2.3796, 2.3789, 2.3786, 2.3783, 2.3781, 2.3771, 2.3755, 2.3755, 2.375, 2.3748, 2.3742, 2.3726, 2.3725, 2.3721, 2.3716, 2.3715, 2.3714, 2.371, 2.3707, 2.3693, 2.3671, 0.3327, 2.448, 2.4468, 2.4458, 2.4448, 2.4444, 2.4441, 2.4435, 2.4427, 2.4426, 2.4419, 2.4396, 2.438, 2.4378, 2.4378, 2.4374, 2.4355, 2.4336, 2.4334, 2.4326, 2.432, 2.4294, 2.4284, 2.4274, 2.4269, 2.4252, 2.4246, 2.4246, 2.4242, 2.4231, 2.4208, 2.408, 0.2401, 2.7696, 2.7694, 2.7692, 2.7691, 2.7683, 2.7677, 2.7665, 2.766, 2.7654, 2.762, 2.7617, 2.7613, 2.7588, 2.7575, 2.7573, 2.757, 2.7565, 2.7564, 2.7556, 2.7555, 2.7542, 2.7532, 2.752, 2.7485, 2.7476, 2.7455, 2.7449, 2.7427, 2.7426, 2.7414, 2.8455, 2.8448, 2.8428, 2.8419, 2.8414, 2.8398, 2.8385, 2.8376, 2.8371, 2.8351, 2.8345, 2.8305, 2.83, 2.8278, 2.8272, 2.8265, 2.8262, 2.8258, 2.8238, 2.8221, 2.8219, 2.8217, 2.8201, 2.8186, 2.8167, 2.8129, 2.812, 2.8105, 2.8102, 2.8029, 3.1242, 3.1216, 3.1207, 3.1167, 3.1156, 3.1151, 3.1111, 3.1105, 3.11, 3.1084, 3.1082, 3.1036, 3.1033, 3.1019, 3.1014, 3.1014, 3.1001, 3.0986, 3.0969, 3.096, 3.0953, 3.0883, 3.0871, 3.0865, 3.0841, 3.0817, 3.0798, 3.079, 3.0784, 3.0776, 3.0706]}, \"token.table\": {\"Topic\": [6, 6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 4, 5, 2, 4, 6, 5, 6, 4, 8, 8, 8, 8, 5, 8, 3, 4, 7, 2, 7, 6, 3, 8, 3, 2, 1, 1, 2, 4, 1, 2, 2, 5, 2, 5, 4, 4, 4, 7, 4, 3, 7, 4, 5, 3, 7, 1, 4, 2, 7, 5, 1, 6, 6, 1, 2, 1, 4, 3, 6, 8, 6, 2, 1, 2, 5, 2, 6, 8, 8, 7, 1, 8, 5, 6, 1, 7, 8, 2, 7, 5, 7, 1, 8, 8, 3, 2, 5, 7, 5, 4, 4, 4, 8, 2, 8, 3, 3, 2, 4, 7, 8, 8, 3, 3, 8, 7, 5, 2, 3, 5, 6, 8, 6, 5, 5, 8, 6, 5, 5, 8, 5, 3, 2, 2, 3, 4, 6, 7, 6, 5, 7, 1, 2, 6, 3, 3, 1, 3, 1, 4, 7, 4, 4, 3, 8, 2, 8, 7, 1, 4, 1, 6, 5, 8, 7, 5, 3, 2, 3, 5, 6, 5, 8, 7, 3, 2, 1, 8, 2, 7, 3, 1, 6, 3, 1, 3, 1, 8, 7, 4, 7, 6, 5, 4, 6, 6, 6, 1, 1, 1, 4, 8, 5, 4, 3, 8, 2, 1, 2, 4, 5, 7, 7, 5, 3, 5, 2, 2, 1, 6, 6, 7, 2, 6, 1, 2, 3, 3, 2, 5, 6, 2, 4, 4, 8, 7, 1, 6, 7, 3, 6, 1, 4, 5, 7, 7, 3, 7, 4, 8, 1], \"Freq\": [0.9963677662926357, 0.9891812177882449, 0.9991348244675797, 0.44107820641841705, 0.17945828986631673, 0.07999947861510505, 0.1275667361700324, 0.11026955160460426, 0.02162148070678515, 0.019459332636106635, 0.02162148070678515, 0.998658804341701, 0.995910545554571, 0.9916765051457972, 0.9858482150741928, 0.9988930294903058, 0.9884308221876584, 0.9872989032265875, 0.9925139477742376, 0.946497667270935, 0.9771194376141489, 0.9685515481071854, 0.971405445200787, 0.9868503140418682, 0.9646461971737169, 0.9705454319176664, 0.9809193710036483, 0.9389212369560789, 0.9989539850565102, 0.9951679276157144, 0.9800808635743967, 0.9903628162341988, 0.966470683580904, 0.9956748724366611, 0.9915804782140866, 0.9892441919810764, 0.9948889767117794, 0.9882762340502047, 0.9874613663437928, 0.9944084052902793, 0.9913126433688745, 0.9969805993989833, 0.975488388998893, 0.9824027881398963, 0.9575757818123933, 0.9947251552528141, 0.9951213097315428, 0.9874032731994729, 0.9935326677843569, 0.9954043101523742, 0.9835564056775578, 0.9947366660796465, 0.9836358926933674, 0.9931414246159139, 0.9878923617192756, 0.9981655686056874, 0.9997680849160844, 0.9933559846776627, 0.9998314834973543, 0.969387770059011, 0.9976845411880637, 0.9999260309521052, 0.9928820211688696, 0.9864958627508046, 0.9881791867188212, 0.987140378587845, 0.9904713193843142, 0.9777336959249553, 0.9887330424981892, 0.971564919952089, 0.9991630477041914, 0.9910364389250383, 0.9699117785547238, 0.9952858174251433, 0.9995153708415849, 0.9943147702901713, 0.9986555893220715, 0.9858780088200685, 0.9587937256167258, 0.9855988104561809, 0.9852548623212316, 0.9925186971475136, 0.9552931761540485, 0.976458358717836, 0.9767879451629717, 0.9948387849260623, 0.981410400995415, 0.9938608764897103, 0.986527164419315, 0.9824037132390552, 0.9884810981971185, 0.972542173770004, 0.9982297539793618, 0.9754772992935281, 0.9333736261406201, 0.9800483279026457, 0.9912657111583625, 0.9994826505233801, 0.9693322968726789, 0.9936805623648601, 0.9778283769110037, 0.9977559248587166, 0.9787794181016714, 0.9724978748506536, 0.9687839742056622, 0.9884278147401455, 0.9914351395483577, 0.9910629457531426, 0.9716472097123483, 0.9867656225987724, 0.9526007765771919, 0.9931372784361719, 0.9590517887005334, 0.9831131560518607, 0.9763859683608486, 0.986745743767532, 0.9903618935124426, 0.9697636079796175, 0.9864038351641954, 0.9978792837776083, 0.9829814873983409, 0.996400652027752, 0.941828471797138, 0.9875965915130819, 0.9831338338306159, 0.9696516203122573, 0.9549414184993414, 0.9930360766313673, 0.9982163248436815, 0.9975887639119603, 0.9854136606094025, 0.9816674350459348, 0.9816979995388211, 0.9895413832323434, 0.9774633260600568, 0.9993002616519819, 0.9855491124186468, 0.9928319855774291, 0.9959815476592028, 0.9915155396088001, 0.9648178794148539, 0.9801559170902313, 0.99046178120694, 0.9826530055163947, 0.9845146652660546, 0.955808977201369, 0.9963830983474835, 0.9981242500061284, 0.9924507693455819, 0.9841217437854141, 0.9822494113115927, 0.9657388369859691, 0.9835215893928518, 0.99583779748185, 0.9886967910634561, 0.9931408107227258, 0.9992271501870921, 0.971805533526105, 0.9903870383679808, 0.9936261839370379, 0.9990827431065581, 0.9984924711277987, 0.9894821862795153, 0.992954434858492, 0.9784666003667246, 0.9959553809740277, 0.9876433688139287, 0.9988473415856127, 0.9663125816597827, 0.9988058980081592, 0.9891338626891228, 0.9985057564859078, 0.9700190987022833, 0.9833816443920349, 0.9945802485634729, 0.9628470381750519, 0.9847710264284554, 0.9859654696205722, 0.9811159152432605, 0.991914927180751, 0.9594132315243931, 0.9882534077314425, 0.9897267503863735, 0.9794735183862902, 0.9764517073615144, 0.9937479710123245, 0.9655417876178265, 0.9891340121783424, 0.9686434030384035, 0.9801387392109165, 0.9937755249391049, 0.9797054887802805, 0.9680965658051932, 0.9974805506777913, 0.9964922400598932, 0.9959098727726123, 0.987683905440053, 0.9823984788865174, 0.9989696353909725, 0.9977371576306212, 0.9975991949991768, 0.9878061927478969, 0.9956675461866664, 0.9843270149365153, 0.9951283925517794, 0.97382238711891, 0.9816298653644786, 0.9925889811970826, 0.9980791278513433, 0.9884125272606695, 0.9893880773368467, 0.9827286767434005, 0.9768614659212176, 0.9889521034457889, 0.9838758048347458, 0.9883009628644536, 0.9881947413913649, 0.9641225449231119, 0.9973869259756835, 0.9999316884353178, 0.9985063672291176, 0.9973988157009924, 0.9970957537963446, 0.9947363398674066, 0.9827484420922127, 0.9956016097840803, 0.9866662071257714, 0.9760901162832668, 0.9800533581234181, 0.9903435144708351, 0.9785920169678909, 0.9553095025466942, 0.9884768469556101, 0.9978948781607224, 0.9889435997932823, 0.9916770717308184, 0.9663365039910445, 0.9956070174465401, 0.9805797793285145, 0.9993758180627415, 0.9937627308364255, 0.9952467642264232, 0.9970356809727267, 0.9992416314857983, 0.9987836091553649, 0.9903999162233126, 0.9785221026792107, 0.9992294441475114, 0.9932129219556108, 0.9784532179172101, 0.9811541800613753, 0.9971853464032657], \"Term\": [\"ab\", \"aber\", \"afd\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"afd_fordert\", \"al\", \"alle\", \"aller\", \"anderen\", \"auch\", \"auf\", \"aufgrund\", \"ausbauen\", \"ausgebaute\", \"ausgestattet\", \"auslandische\", \"bedeutet\", \"beenden\", \"befordert\", \"begegnen\", \"begrenzen\", \"begru\\u00dfen\", \"bei\", \"beim\", \"beispielsweise\", \"bekampfen\", \"belastet\", \"bereit\", \"beschleunigen\", \"besonder\", \"besser\", \"bestehende\", \"besteht\", \"bewahren\", \"bis\", \"bleiben\", \"braucht\", \"bringen\", \"bundesweiten\", \"bzw\", \"dabei\", \"dafur\", \"daher\", \"damit\", \"daraus\", \"darf\", \"dass\", \"dazu\", \"dazu_gehort\", \"dem\", \"den\", \"denen\", \"der\", \"des\", \"deutsche\", \"die\", \"diese\", \"diesem\", \"dieser\", \"diesis\", \"digitale\", \"digitalen\", \"digitaler\", \"dort\", \"durch\", \"durfen\", \"ebenso\", \"ein\", \"eine\", \"einen\", \"einer\", \"einfuhren\", \"eingehalten\", \"einsetzen\", \"entgegenwirken\", \"entscheiden\", \"entschieden\", \"entsprechend\", \"entsprechende\", \"erhalten\", \"ersetzen\", \"es\", \"etwa\", \"europaischen\", \"europaischer\", \"fachliche\", \"fordern\", \"fordert\", \"forderungen\", \"freiheitliche\", \"fuhren\", \"fur\", \"garantierten\", \"geben\", \"gefahrdet\", \"gegen\", \"gehoren\", \"gemeinsaman\", \"generelle\", \"gerecht\", \"gescheitert\", \"gesetzliche\", \"gewahrleisten\", \"gewahrleistet\", \"gezeigt\", \"gild\", \"gilt\", \"globale\", \"grundlegende\", \"gut\", \"gute\", \"guten\", \"guter\", \"haben\", \"halten\", \"hat\", \"haufig\", \"hinaus\", \"hoch\", \"hoheren\", \"ihnen\", \"ihre\", \"ihrer\", \"immer\", \"in\", \"indem\", \"integration\", \"internationalen\", \"internet\", \"ist\", \"jede\", \"kann\", \"kapitel\", \"keine\", \"km\", \"komman\", \"konnen\", \"konsequent\", \"konsequente\", \"langer\", \"lehnen\", \"lehnt\", \"leisten\", \"letzten\", \"liegt\", \"machen\", \"man\", \"mehr\", \"migration\", \"mindesten\", \"mit\", \"moglich\", \"moglichst\", \"muss\", \"mussen\", \"nach\", \"nationaler\", \"nehman\", \"nehmen\", \"neue\", \"neuen\", \"nicht\", \"notwendigen\", \"nur\", \"nutzen\", \"oder\", \"offentlich\", \"offentliche\", \"ohne\", \"orientiert\", \"pandemie\", \"politische\", \"profitieren\", \"rahman\", \"rechtliche\", \"reformieren\", \"regionaler\", \"richtet\", \"ring\", \"schaffen\", \"schafft\", \"schutzen\", \"sehen\", \"sehr\", \"seine\", \"seiner\", \"selbst\", \"setzen\", \"setzt\", \"sich\", \"sichern\", \"sie\", \"sind\", \"soll\", \"sollen\", \"sorgen\", \"sowie\", \"sozialen\", \"sozialer\", \"spricht\", \"staatlich\", \"staatliche\", \"starken\", \"starker\", \"statt\", \"stattdessen\", \"stellt\", \"strikt\", \"treten\", \"uber\", \"ubernehman\", \"umgehend\", \"un\", \"und\", \"unsere\", \"unserer\", \"unter\", \"unterstutzen\", \"unterstutzt\", \"verbessern\", \"verbraucher\", \"verfolgen\", \"verhindern\", \"verhindert\", \"verpflichtende\", \"vollstandige\", \"vom\", \"vor\", \"vor_allem\", \"wahrend\", \"weil\", \"weiter\", \"weitere\", \"werden\", \"wie\", \"wieder\", \"wir\", \"wird\", \"wollen\", \"zeigen\", \"zerstort\", \"zu\", \"zum\", \"zunehmend\", \"zunehmende\", \"zur\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 7, 5, 1, 3, 4, 6, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el65948117772208966856037143\", ldavis_el65948117772208966856037143_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el65948117772208966856037143\", ldavis_el65948117772208966856037143_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el65948117772208966856037143\", ldavis_el65948117772208966856037143_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "7     -0.289821  0.334409       1        1  37.617130\n",
       "6      0.129761  0.394234       2        1  18.597044\n",
       "4      0.375152 -0.149635       3        1   9.493408\n",
       "0     -0.337992 -0.031103       4        1   9.193421\n",
       "2      0.256156  0.113598       5        1   8.643080\n",
       "3     -0.020222 -0.033884       6        1   6.257162\n",
       "5     -0.204795 -0.304741       7        1   5.805745\n",
       "1      0.091762 -0.322878       8        1   4.393010, topic_info=             Term          Freq         Total Category  logprob  loglift\n",
       "3             die  11791.000000  11791.000000  Default  30.0000  30.0000\n",
       "73            der   6180.000000   6180.000000  Default  29.0000  29.0000\n",
       "39            und   6562.000000   6562.000000  Default  28.0000  28.0000\n",
       "62            fur   2044.000000   2044.000000  Default  27.0000  27.0000\n",
       "165            zu   1686.000000   1686.000000  Default  26.0000  26.0000\n",
       "...           ...           ...           ...      ...      ...      ...\n",
       "511   profitieren     18.507449     19.365704   Topic8  -5.4134   3.0798\n",
       "1180       haufig     18.250544     19.111760   Topic8  -5.4274   3.0790\n",
       "1517        ihnen     17.989130     18.849324   Topic8  -5.4418   3.0784\n",
       "3799    befordert     17.793058     18.659691   Topic8  -5.4528   3.0776\n",
       "3878  eingehalten     18.764725     19.816567   Topic8  -5.3996   3.0706\n",
       "\n",
       "[279 rows x 6 columns], token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "1435      6  0.996368           ab\n",
       "85        6  0.989181         aber\n",
       "7101      1  0.999135          afd\n",
       "9057      1  0.441078  afd_fordert\n",
       "9057      2  0.179458  afd_fordert\n",
       "...     ...       ...          ...\n",
       "165       3  0.999229           zu\n",
       "209       7  0.993213          zum\n",
       "247       4  0.978453    zunehmend\n",
       "2391      8  0.981154   zunehmende\n",
       "387       1  0.997185          zur\n",
       "\n",
       "[252 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 7, 5, 1, 3, 4, 6, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis=gensimvis.prepare(lda_model,corpus,id2word,mds='mmds',R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "def make_trgram(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "pyLDAvis.enable_notebook()\n",
    "import os\n",
    "def create_LDA(text_liste,num_topics,name,allowed_posttags=['NOUN','ADJ','VERB','ADV'],zusatz=''):\n",
    "    lemma_text=lemmatization_test(text_liste,allowed_posttags)\n",
    "    data_words=gen_words(lemma_text)\n",
    "    bigram_phrases=gensim.models.Phrases(data_words,min_count=3,threshold=100)\n",
    "    trigram_phases=gensim.models.Phrases(bigram_phrases[data_words],threshold=50)\n",
    "\n",
    "    bigram=gensim.models.phrases.Phraser(bigram_phrases)\n",
    "    trigram=gensim.models.phrases.Phraser(trigram_phases)\n",
    "\n",
    "    data_bigrams=make_bigrams(data_words)\n",
    "    data_bigrams_trigrams=make_trgram(data_bigrams)\n",
    "  \n",
    "\n",
    "    id2word=corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "    texts = data_bigrams_trigrams\n",
    "\n",
    "    corpus=[id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    print(corpus[0][0:90])\n",
    "\n",
    "    tfidf=TfidfModel(corpus,id2word=id2word)\n",
    "\n",
    "    low_value=0.03\n",
    "    words=[]\n",
    "    words_missing_in_tfdf=[]\n",
    "\n",
    "    for i in tqdm(range(0,len(corpus))):\n",
    "        bow = corpus[i]\n",
    "        low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "        tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "        bow_ids = [id for id, value in bow]\n",
    "        low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "        drops=low_value_words+words_missing_in_tfdf\n",
    "        for item in drops:\n",
    "            words.append(id2word[item])\n",
    "        words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "        new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]  \n",
    "        corpus[i]=new_bow\n",
    "\n",
    "\n",
    "    lda_model=gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha='auto'\n",
    "    )\n",
    "    dic={}\n",
    "    dic_sum={}\n",
    "    for idx, topic in lda_model.print_topics(num_topics,num_words=100):\n",
    "        elements=[]\n",
    "        percent=[]\n",
    "\n",
    "        for e in topic.split('+'):\n",
    "            elements.append(e.split('*')[1].replace('\"','').strip())\n",
    "            percent.append(float(e.split('*')[0].replace('\"','').strip()))\n",
    "\n",
    "        dic[str(idx)+'_word']=elements\n",
    "        dic[str(idx)+'_per']=percent\n",
    "        dic_sum[idx]=sum(percent)\n",
    "    df_topic=pd.DataFrame(data=dic)\n",
    "    pd.DataFrame(data=dic).to_csv(f'LDA/{name}{zusatz}_{num_topics}.csv')\n",
    "  \n",
    "    LDAvis_data_filepath = os.path.join(f'LDA/pic/{name}'+zusatz+str(num_topics))\n",
    "    # # this is a bit time consuming - make the if statement True\n",
    "    # # if you want to execute visualization prep yourself\n",
    "    if 1 == 1:\n",
    "        LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word,mds='mmds',R=30)\n",
    "        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "            pickle.dump(LDAvis_prepared, f)\n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, f'LDA/pic/{name}'+zusatz+ str(num_topics) +'.html')\n",
    "    LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIE_GRUENEN_Wahlprogramm_2021',\n",
       " 'FDP_Wahlprogramm_2021',\n",
       " 'DIE_LINKE_Wahlprogramm_2021',\n",
       " 'CDU-CSU_Wahlrprogramm_2021',\n",
       " 'SPD_Wahlprogramm_2021',\n",
       " 'AFD_Wahlprogramm_2021']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dic_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IEITTESALTENTR Alles ist drin',\n",
       " 'Bundestagswahlprogramm',\n",
       " 'Bereit, weil Ihr es seid',\n",
       " 'Dieses Bundestagswahlprogramm wurde auf der  Bundesdelegiertenkonferenz von BÜNDNIS DIE GRÜNEN beschlossen, die vom bis  Juni digital stattgefunden hat',\n",
       " 'Herausgeberin:  BÜNDNIS DIE GRÜNEN Platz vor dem Neuen Tor',\n",
       " 'Berlin  Telefon:  Fax: E-Mail: info@gruene',\n",
       " 'de Internet: gruene',\n",
       " 'de  V',\n",
       " ':  BÜNDNIS DIE GRÜNEN Annkathrin Schäfer  Platz vor dem Neuen Tor',\n",
       " 'Berlin  Layout und Satz: Twentyfour Seven Creative Media Service GmbH, Berlin twentyfour-/']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_list['DIE_GRUENEN_Wahlprogramm_2021'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29cba453f9f42ec8fb45eeb20a7c57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c3dd592320423f892b4076aac7a552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6128e012cc5a4f9a925c71a5a7a14a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618ba3cd03494c2b816141faaa4d6666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b63c343a11f468dba626d086c34160d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd3feb6be1e4cd8a2ec035c27670c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10636d41778948c98ef33795136de809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n",
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e07b3c0c8e4ab496071f45a8faabd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d840007aba5412e99868cc4af9b26ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a4948c190549389a532d53db0fcfc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88226fcff06d4858a8af5e8c59b4b892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136acb625d524478a149e5ed9eeb0007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97f6f5e59ef493b8f8e50caccc31ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6017c84b8a44b5b084b63ec37640e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168763b638b84df4b95ccbe7864c0ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce8e3037b8d4d408a2e9aa4876ed4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77b71df368541c8bc0e486ec08504e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0725b7eb68a54137bf974ec4d3cbbc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe47358c7c4a486a833ec5ebb50403da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(list(dic_list.keys())):\n",
    "    create_LDA(dic_list['DIE_GRUENEN_Wahlprogramm_2021'],num_topics,name,allowed_posttags=['NOUN','ADJ','VERB','ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2b53e8fc1e4940b61ca27549a0d6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f82fecbc7324d8f81f333dfa8046190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2547514c2dc4642b71ccedd22208c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d17bd459554e058abedf53dce42e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6ada27eb0a4ff19d45bc90cc0ae032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306c2df9af0b4265becb81b2e4430829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00158f4dd4044918fa700164769ced7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ebb8c3225c43b9a97e720d72f2513e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe20b6df9f8461095b6a42434c0e489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c25e2c412764e2796abff45a055932c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9016df687ed24b6c860a87d5dcd03b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b8c31f80434937bc736afa85366d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9726800fa8224b8f8a3ecf5fe3c3d554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8f957c49ac471e8bd1e1208b0aad26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c1b26a50174887a125df969b491c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ceafa27aef94a8a8fcae87a5a1acf70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17924172c884c86a761fa4f8f107851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a303ba7b7c694ce5b838b10381027eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8a1243eaa94768a6659417e0179471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(list(dic_list.keys())):\n",
    "    create_LDA(dic_list['DIE_GRUENEN_Wahlprogramm_2021'],num_topics,name,allowed_posttags=['NOUN','VERB'],zusatz='_verb_and_nome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54c35701683d52397dec8c1b1dabb7d7599bf83c9aec0584240eb3e578830d84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
