{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='data/AFD_Wahlprogramm_2021.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename) as f:\n",
    "    text = f.read()\n",
    "    #text = text.replace(r'\\\\d+\\\\n', '$')\n",
    "    #text=   re.sub(\"\\n\\n\", \".\", text)\n",
    "    #text=   re.sub(\"\\d+.\", \".\", text)\n",
    "    text=   re.sub(\" \\d+\\n\", \".\", text)\n",
    "    text=   re.sub(\"\\n\\d+\", \" \", text)\n",
    "    text=   re.sub(\"\\n\", \" \", text)\n",
    "sentences = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "\n",
    "def load_yaml_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads a yaml file and returns a dictionary with the contents.\n",
    "    Args:\n",
    "        file_path (str):\n",
    "            Path to the yaml file\n",
    "    Returns:\n",
    "        yaml_dict:\n",
    "            Dictionary with the contents of the yaml file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as stream:\n",
    "        yaml_dict = yaml.safe_load(stream)\n",
    "        return yaml_dict\n",
    "\n",
    "\n",
    "def load_markdown_file(file_path):\n",
    "    \"\"\"\n",
    "    Loads a markdown file and returns a string with the contents.\n",
    "    Args:\n",
    "        file_path (str):\n",
    "            Path to the markdown file\n",
    "    Returns:\n",
    "        markdown_str:\n",
    "            String with the contents of the markdown file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as stream:\n",
    "        markdown_str = stream.read()\n",
    "        return markdown_str\n",
    "\n",
    "\n",
    "def get_counts(df: pd.DataFrame, label_col: str = \"label\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the labels count in a dataframe.\n",
    "    Args:\n",
    "        df (pd.DataFrame):\n",
    "            Dataframe to get the counts from.\n",
    "        label_col (str):\n",
    "            Column name of the label column.\n",
    "    Returns:\n",
    "        counts_df:\n",
    "            Dataframe with the counts.\n",
    "    \"\"\"\n",
    "    count_df = df[label_col].value_counts().to_frame().reset_index()\n",
    "    count_df.columns = [label_col, \"sentence_count\"]\n",
    "    count_df[\"percent\"] = count_df.sentence_count / count_df.sentence_count.sum() * 100\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import spacy\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def _add_sentence_to_list(sentence: str, sentences_list):\n",
    "    \"\"\"\n",
    "    Add a sentence to the list of sentences.\n",
    "    Args:\n",
    "        sentence (str):\n",
    "            Sentence to be added.\n",
    "        sentences (List[str]):\n",
    "            List of sentences.\n",
    "    \"\"\"\n",
    "    while sentence.startswith(\" \"):\n",
    "        # remove leading space\n",
    "        sentence = sentence[1:]\n",
    "    if all(c in punctuation for c in sentence) or len(sentence) == 1:\n",
    "        # skip sentences with only punctuation\n",
    "        return\n",
    "    sentences_list.append(sentence)\n",
    "\n",
    "\n",
    "def get_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Get sentences from a text.\n",
    "    Args:\n",
    "        text (str):\n",
    "            Text to be processed.\n",
    "    Returns:\n",
    "        List[str]:\n",
    "            List of sentences.\n",
    "    \"\"\"\n",
    "    # get the paragraphs\n",
    "    paragraphs = text.split(\"\\n\")\n",
    "    paragraphs = [p for p in paragraphs if p != \"\"]\n",
    "    # get the sentences from the paragraphs\n",
    "    sentences = list()\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.startswith(\"#\"):\n",
    "            _add_sentence_to_list(paragraph, sentences)\n",
    "            continue\n",
    "        prev_sentence_idx = 0\n",
    "        for idx in range(len(paragraph)):\n",
    "            if idx + 1 < len(paragraph):\n",
    "                if (paragraph[idx] == \".\" and not paragraph[idx + 1].isdigit()) or (\n",
    "                    paragraph[idx] in \"!?\"\n",
    "                ):\n",
    "                    sentence = paragraph[prev_sentence_idx : idx + 1]\n",
    "                    _add_sentence_to_list(sentence, sentences)\n",
    "                    prev_sentence_idx = idx + 1\n",
    "            else:\n",
    "                sentence = paragraph[prev_sentence_idx:]\n",
    "                _add_sentence_to_list(sentence, sentences)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_words(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get every word in the text that isn't a stopword or punctuation,\n",
    "    and that is either a noun, adjective, verb or interjection\n",
    "    (based on the [universal POS tags](https://universaldependencies.org/u/pos/))\n",
    "    Args:\n",
    "        text (str):\n",
    "            Text to be processed.\n",
    "    Returns:\n",
    "        List[str]:\n",
    "            List of words.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    nlp.max_length = 8000000\n",
    "    doc = nlp(text)\n",
    "    words = [\n",
    "        word.text.replace(\"\\n\", \"\").replace(\"*\", \"\")  # remove new line and bold symbols\n",
    "        for word in doc\n",
    "        if not word.is_stop  # remove stopwords\n",
    "        and not word.is_punct  # remove punctuation\n",
    "        and (\n",
    "            word.pos_ == \"NOUN\"  # noun\n",
    "            or word.pos_ == \"ADJ\"  # adjective\n",
    "            or word.pos_ == \"VERB\"  # verb\n",
    "            or word.pos_ == \"INTJ\"  # interjection\n",
    "            or word.pos_ == \"X\"  # other\n",
    "        )\n",
    "    ]\n",
    "    # remove blank words and spaces\n",
    "    words = [word for word in words if word != \"\"]\n",
    "    words = [word.replace(\" \", \"\") for word in words]\n",
    "    # make all words lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    # remove undesired words\n",
    "    words = [\n",
    "        word\n",
    "        for word in words\n",
    "        if word not in [\"se\", \"há\", \"política\", \"político\", \"políticos\", \"políticas\"]\n",
    "    ]\n",
    "    # remove words with less than 3 characters\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    return words\n",
    "\n",
    "\n",
    "def get_topical_sentences(\n",
    "    sentences, topics\n",
    ") :\n",
    "    \"\"\"\n",
    "    Get lists of sentences per topic, based on the presence of\n",
    "    words that are a part of the topic.\n",
    "    Args:\n",
    "        sentences (List[str]):\n",
    "            List of sentences to analyse.\n",
    "        topics (Dict[str, List[str]]):\n",
    "            Dictionary of words per topic.\n",
    "    Returns:\n",
    "        Dict[str, List[str]]:\n",
    "            Dictionary of sentences per topic.\n",
    "    \"\"\"\n",
    "    topical_sentences = dict()\n",
    "    for topic in topics:\n",
    "        topical_sentences[topic] = list()\n",
    "    for sentence in sentences:\n",
    "        for topic in topics:\n",
    "            if any(topical_word in sentence.lower() for topical_word in topics[topic]):\n",
    "                topical_sentences[topic].append(sentence)\n",
    "    return topical_sentences\n",
    "\n",
    "\n",
    "def get_sentiment(sentences):\n",
    "    \"\"\"\n",
    "    Get the sentiment of a list of sentences.\n",
    "    Args:\n",
    "        sentences (str):\n",
    "            List of sentences to analyse.\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            Sentiment of the sentences.\n",
    "    \"\"\"\n",
    "    sentiment_model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "    sentiment_task = pipeline(\n",
    "        \"sentiment-analysis\", model=sentiment_model_path, tokenizer=sentiment_model_path\n",
    "    )\n",
    "    sentiment_outputs = [\n",
    "        sentiment_task(sentence)\n",
    "        for sentence in tqdm(sentences, desc=\"Sentiment analysis\")\n",
    "    ]\n",
    "    sentiments_dict = dict(label=[], score=[], sentence=[])\n",
    "    for idx, output in enumerate(sentiment_outputs):\n",
    "        sentiments_dict[\"label\"].append(output[0][\"label\"])\n",
    "        sentiments_dict[\"score\"].append(output[0][\"score\"])\n",
    "        sentiments_dict[\"sentence\"].append(sentences[idx])\n",
    "    sentiment_df = pd.DataFrame(sentiments_dict)\n",
    "    sentiment_df[\"label\"] = sentiment_df.label.map(\n",
    "        dict(Positive=\"positivo\", Negative=\"negativo\", Neutral=\"neutro\")\n",
    "    )\n",
    "    return sentiment_df\n",
    "\n",
    "\n",
    "def get_hate_speech(\n",
    "    sentences, sentiment_df, label_col: str = \"label\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the hate speech of a list of sentences.\n",
    "    Args:\n",
    "        sentences (str):\n",
    "            List of sentences to analyse.\n",
    "        sentiment_df (pd.DataFrame):\n",
    "            Sentiment of the sentences.\n",
    "        label_col (str):\n",
    "            Column of the sentiment dataframe that contains the sentiment.\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            Hate speech of the sentences.\n",
    "    \"\"\"\n",
    "    hate_model_path = \"Hate-speech-CNERG/dehatebert-mono-portugese\"\n",
    "    hate_task = pipeline(\n",
    "        \"text-classification\", model=hate_model_path, tokenizer=hate_model_path\n",
    "    )\n",
    "    hate_outputs = [\n",
    "        hate_task(sentence) for sentence in tqdm(sentences, desc=\"Hate speech analysis\")\n",
    "    ]\n",
    "    hate_dict = dict(label=[], score=[], sentence=[])\n",
    "    for idx, output in enumerate(hate_outputs):\n",
    "        hate_dict[\"label\"].append(output[0][\"label\"])\n",
    "        hate_dict[\"score\"].append(output[0][\"score\"])\n",
    "        hate_dict[\"sentence\"].append(sentences[idx])\n",
    "    hate_df = pd.DataFrame(hate_dict)\n",
    "    hate_df[\"label\"] = hate_df.label.map(dict(HATE=\"ódio\", NON_HATE=\"neutro\"))\n",
    "    hate_condition = (hate_df.label == \"ódio\") & (sentiment_df[label_col] == \"negativo\")\n",
    "    hate_df.loc[hate_condition, \"label\"] = \"ódio\"\n",
    "    hate_df.loc[~hate_condition, \"label\"] = \"neutro\"\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.graph_objects import Figure\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def get_word_cloud(\n",
    "    words: List[str],\n",
    "    max_words: int = 500,\n",
    "    image_path: str = None,\n",
    "    image_name: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a word cloud based on a set of words.\n",
    "    Args:\n",
    "        words (List[str]):\n",
    "            List of words to be included in the word cloud.\n",
    "        max_words (int):\n",
    "            Maximum number of words to be included in the word cloud.\n",
    "        image_path (str):\n",
    "            Path to the image file where to save the word cloud.\n",
    "        image_name (str):\n",
    "            Name of the image where to save the word cloud.\n",
    "    \"\"\"\n",
    "    # change the value to black\n",
    "    def black_color_func(\n",
    "        word, font_size, position, orientation, random_state=None, **kwargs\n",
    "    ):\n",
    "        return \"hsl(0,100%, 1%)\"\n",
    "\n",
    "    # set the wordcloud background color to white\n",
    "    # set width and height to higher quality, 3000 x 2000\n",
    "    wordcloud = WordCloud(\n",
    "        font_path=\"/Library/Fonts/Arial Unicode.ttf\",\n",
    "        background_color=\"white\",\n",
    "        width=3000,\n",
    "        height=2000,\n",
    "        max_words=max_words,\n",
    "    ).generate(\" \".join(words))\n",
    "    # set the word color to black\n",
    "    wordcloud.recolor(color_func=black_color_func)\n",
    "    # set the figsize\n",
    "    plt.figure(figsize=[15, 10])\n",
    "    # plot the wordcloud\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    # remove plot axes\n",
    "    plt.axis(\"off\")\n",
    "    if image_path is not None and image_name is not None:\n",
    "        # save the image\n",
    "        plt.savefig(os.path.join(image_path, image_name), bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "def plot_topical_presence(\n",
    "    sentences: List[str],\n",
    "    topics: Dict[str, List[str]],\n",
    "    title: str = None,\n",
    "    color: str = \"blue\",\n",
    "    height: int = 300,\n",
    ") :\n",
    "    \"\"\"\n",
    "    Plot the number of sentences per topic.\n",
    "    Args:\n",
    "        sentences (List[str]):\n",
    "            List of sentences to analyse.\n",
    "        topics (Dict[str, List[str]]):\n",
    "            Dictionary of words per topic.\n",
    "        title (str):\n",
    "            Title of the plot.\n",
    "        color (str):\n",
    "            Color of the bars in the plot.\n",
    "        height (int):\n",
    "            Height of the plot.\n",
    "    Returns:\n",
    "        Figure:\n",
    "            Plotly figure with the number of sentences per topic.\n",
    "    \"\"\"\n",
    "    topical_sentences = get_topical_sentences(sentences, topics)\n",
    "    topic_sentence_count = dict()\n",
    "    for topic in topical_sentences.keys():\n",
    "        topic_sentence_count[topic] = len(topical_sentences[topic])\n",
    "    topic_sentence_count = pd.DataFrame(\n",
    "        topic_sentence_count, index=[\"sentence_count\"]\n",
    "    ).T\n",
    "    topic_sentence_count[\"sentence_percentage\"] = (\n",
    "        topic_sentence_count[\"sentence_count\"] / len(sentences) * 100\n",
    "    )\n",
    "    topic_sentence_count.index.name = \"topic\"\n",
    "    topic_sentence_count.sort_index(inplace=True)\n",
    "    fig = px.bar(topic_sentence_count, x=\"sentence_percentage\", orientation=\"h\")\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Percentagem de frases topicais no texto\",\n",
    "        yaxis_title=\"Tópico\",\n",
    "        yaxis=dict(categoryorder=\"category descending\"),\n",
    "        margin=dict(l=0, r=0, b=0, t=0, pad=0),\n",
    "        height=height,\n",
    "    )\n",
    "    fig.update_traces(marker_color=color)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_approaches(\n",
    "    sentences: List[str],\n",
    "    approaches: Dict[str, List[str]],\n",
    "    title: str = None,\n",
    "    height: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the approaches taken to language and policy.\n",
    "    Args:\n",
    "        sentences (List[str]):\n",
    "            List of sentences to analyse.\n",
    "        approaches (Dict[str, List[str]]):\n",
    "            Dictionary of words per approach.\n",
    "        title (str):\n",
    "            Title of the plot.\n",
    "        height (int):\n",
    "            Height of the plot.\n",
    "    Returns:\n",
    "        Figure:\n",
    "            Plotly figure with the number of sentences per approach.\n",
    "    \"\"\"\n",
    "    approach_sentences = get_topical_sentences(sentences, approaches)\n",
    "    approach_sentence_count = dict()\n",
    "    total_num_sentences_in_approaches = sum(\n",
    "        [len(approach_sentences[approach]) for approach in approach_sentences.keys()]\n",
    "    )\n",
    "    for approach in approaches:\n",
    "        approach_sentence_count[approach] = (\n",
    "            len(approach_sentences[approach]) / total_num_sentences_in_approaches * 100\n",
    "        )\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[approach_sentence_count[\"rationality\"]],\n",
    "            name=\"racionalidade\",\n",
    "            orientation=\"h\",\n",
    "            marker=dict(color=\"green\"),\n",
    "            hovertemplate=\"racionalidade: %{x:.1f}%<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[approach_sentence_count[\"intuition\"]],\n",
    "            name=\"intuição\",\n",
    "            orientation=\"h\",\n",
    "            marker=dict(color=\"red\"),\n",
    "            hovertemplate=\"intuição: %{x:.1f}%<extra></extra>\",\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        barmode=\"stack\",\n",
    "        xaxis=dict(\n",
    "            showgrid=False,  # thin lines in the background\n",
    "            zeroline=False,  # thick line at x=0\n",
    "            visible=False,  # numbers below\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showgrid=False,  # thin lines in the background\n",
    "            zeroline=False,  # thick line at x=0\n",
    "            visible=False,  # numbers below\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=0, pad=0),\n",
    "        height=height,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_sentiment(\n",
    "    df: pd.DataFrame, title: str = None, height: int = 300, label_col: str = \"label\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the predicted sentiment of the sentences.\n",
    "    Args:\n",
    "        df (pd.DataFrame):\n",
    "            Dataframe with the outputs of a sentiment analysis model.\n",
    "        title (str):\n",
    "            Title of the plot.\n",
    "        height (int):\n",
    "            Height of the plot.\n",
    "        label_col (str):\n",
    "            Column name of the sentiment.\n",
    "    Returns:\n",
    "        Figure:\n",
    "            Plotly figure with the percentage of hate speech.\n",
    "    \"\"\"\n",
    "    sentiments_count = get_counts(df, label_col=label_col)\n",
    "    labels_order = [\"neutro\", \"positivo\", \"negativo\"]\n",
    "    fig = px.bar(\n",
    "        x=labels_order,\n",
    "        y=[\n",
    "            float(sentiments_count[sentiments_count[label_col] == label].percent)\n",
    "            for label in labels_order\n",
    "        ],\n",
    "        title=title,\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        marker_color=[\"gray\", \"green\", \"red\"],\n",
    "        hovertemplate=\"%{y:.1f}%<extra></extra>\",\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Sentimento\",\n",
    "        yaxis_title=\"Percentagem de frases\",\n",
    "        margin=dict(l=0, r=0, b=0, t=0, pad=0),\n",
    "        height=height,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_hate_speech(\n",
    "    df: pd.DataFrame, title: str = None, height: int = 300, label_col: str = \"label\"\n",
    ") :\n",
    "    \"\"\"\n",
    "    Show the percentage of estimated hate speech sentences.\n",
    "    Args:\n",
    "        df (pd.DataFrame):\n",
    "            Dataframe with the outputs of a hate speech model.\n",
    "        title (str):\n",
    "            Title of the plot.\n",
    "        height (int):\n",
    "            Height of the plot.\n",
    "        label_col (str):\n",
    "            Column name of the hate speech.\n",
    "    Returns:\n",
    "        Figure:\n",
    "            Plotly figure with the percentage of hate speech.\n",
    "    \"\"\"\n",
    "    hate_count = get_counts(df, label_col=label_col)\n",
    "    try:\n",
    "        hate_percent = hate_count[hate_count[label_col] == \"ódio\"].percent.values[0]\n",
    "    except IndexError:\n",
    "        hate_percent = 0\n",
    "    fig = go.Figure(\n",
    "        go.Indicator(\n",
    "            mode=\"number\",\n",
    "            value=hate_percent,\n",
    "            title=title,\n",
    "            number=dict(suffix=\"%\", valueformat=\".2f\"),\n",
    "            delta=dict(position=\"top\", reference=320),\n",
    "            domain=dict(x=[0, 1], y=[0, 1]),\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor=\"darkred\",\n",
    "        font_color=\"white\",\n",
    "        margin=dict(l=0, r=0, b=0, t=0, pad=0),\n",
    "        height=height,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from ipywidgets import interact\n",
    "import plotly.io as pio\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['wirtschaft', 'Klima', 'Bildung', 'Gesundheit', 'Wissenschaft', 'soziale Ursachen', 'Politik und Ideologie', 'Infrastruktur'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = load_yaml_file('topic_modeling/topic_g.yml')\n",
    "topics.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wirtschaft',\n",
       " 'Volkswirtschaft',\n",
       " 'Marktwirtschaft',\n",
       " 'Planwirtschaft',\n",
       " 'Soziale Marktwirtschaft',\n",
       " 'Kapitalismus',\n",
       " 'Kommunismus',\n",
       " 'Sozialismus',\n",
       " 'BIP (Bruttoinlandsprodukt)',\n",
       " 'BIP pro Kopf']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics['wirtschaft'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approaches = load_yaml_file(os.path.join(DATA_DIR, data_name, \"approaches.yml\"))\n",
    "approaches.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54c35701683d52397dec8c1b1dabb7d7599bf83c9aec0584240eb3e578830d84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
