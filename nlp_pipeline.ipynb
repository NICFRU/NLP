{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/DIE_GRUENEN_Wahlprogramm_2021_sent_lemma_list.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "    sent = content.split('ยง')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14956"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data(path: str, rename_dict, drop_list=[]):\n",
    "\n",
    "    def normalize(data, columns):\n",
    "        for col in columns:\n",
    "            data[col] = data[col].str.lower().replace(\"\\\\n\", \".\")\n",
    "        return data\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.rename(columns=rename_dict)\n",
    "    data = data.drop(drop_list, axis=1)\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index()\n",
    "    data = data.drop(\"index\", axis=1)\n",
    "    data = normalize(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenization(data, col):\n",
    "    # Lists to store the results during the calculations\n",
    "    sentences = []\n",
    "    lengths_of_sentences = []\n",
    "\n",
    "    # Initialize names for columns\n",
    "    col_name = col + \"_sent_tok\"\n",
    "    col_name_length = col + \"_num_of_sent\"\n",
    "\n",
    "    # Iterating through the data row for row splitting the text in sentences and counting them\n",
    "    for entry in tqdm(data[col]):\n",
    "        col_sentences = nltk.sent_tokenize(entry)\n",
    "        sentences.append(col_sentences)\n",
    "        lengths_of_sentences.append(len(col_sentences))\n",
    "    \n",
    "    # Insert results to dataframe\n",
    "    data[col_name] = sentences\n",
    "    data[col_name_length] = lengths_of_sentences\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hallo, ich bin Niklas.', 'Wer genau bist du?', 'Wie kommst du hier her?', 'Ich hoffe, es geht dir gut!']\n",
      "['Ja hi, mir gehts gut!', 'Und dir?']\n"
     ]
    }
   ],
   "source": [
    "for entry in [\"Hallo, ich bin Niklas. Wer genau bist du? Wie kommst du hier her? Ich hoffe, es geht dir gut!\", \"Ja hi, mir gehts gut! Und dir?\"]:\n",
    "    sentence = nltk.sent_tokenize(entry)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(data, col):\n",
    "    # Lists to store the results during the calculations\n",
    "    words = []\n",
    "    length_of_words = []\n",
    "\n",
    "    # Initialize names for columns\n",
    "    col_name = col + \"_word_tok\"\n",
    "    col_name_length = col + \"_num_of_word\"\n",
    "\n",
    "    # Iterating through the data row for row splitting the text in words and counting them\n",
    "    for i in tqdm(range(len(data))):\n",
    "        word_list = nltk.regexp_tokenize(data[col][i], pattern='\\w+')\n",
    "        words.append(word_list)\n",
    "        length_of_words.append(len(word_list))\n",
    "\n",
    "    # Insert results to dataframe\n",
    "    data[col_name] = words\n",
    "    data[col_name_length] = length_of_words\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_lemming(data, col):\n",
    "    # Initialize Stemmer and Lemmatizer\n",
    "    pst = nltk.PorterStemmer()\n",
    "    wlem = nltk.WordNetLemmatizer()\n",
    "\n",
    "    # Lists to store results\n",
    "    stems = []\n",
    "    lemms = []\n",
    "\n",
    "    # Initialize names for columns\n",
    "    col_name_stems = col + \"_stems\"\n",
    "    col_name_lemms = col + \"_lemms\"\n",
    "\n",
    "    # Iterating through the data row for row creating stems and lemms\n",
    "    for i in tqdm(range(len(data))):\n",
    "        stem_cache = []\n",
    "        lemm_cache = []\n",
    "\n",
    "        for c in data[col][i]:\n",
    "            stem_cache.append(pst.stem(c))\n",
    "            lemm_cache.append(wlem.lemmatize(c))\n",
    "        \n",
    "        stems.append(stem_cache)\n",
    "        lemms.append(lemm_cache)\n",
    "\n",
    "    # Insert results to dataframe\n",
    "    data[col_name_stems] = stems\n",
    "    data[col_name_lemms] = lemms\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_count_and_removal(data, col, language=\"english\"):\n",
    "\n",
    "    stoplist = nltk.stopwords.words(language)\n",
    "\n",
    "    number_of_stopwords = []\n",
    "    text_without_stopwords = []\n",
    "\n",
    "    col_name_number_of_stopwords = col + \"_num_of_stopwords\"\n",
    "    col_name_without_stopwords = col + \"_without_stopwords\"\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        no_of_words = 0\n",
    "        without_stopwords = []\n",
    "        for word in data[col][i]:\n",
    "            if word in stoplist:\n",
    "                no_of_words += 1\n",
    "            else:\n",
    "                without_stopwords.append(word)\n",
    "        \n",
    "        text_without_stopwords.append(without_stopwords)\n",
    "        number_of_stopwords.append(no_of_words)\n",
    "\n",
    "    data[col_name_number_of_stopwords] = number_of_stopwords\n",
    "    data[col_name_without_stopwords] = text_without_stopwords\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
