{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data(path: str, rename_dict, drop_list=[]):\n",
    "\n",
    "    def normalize(data, columns):\n",
    "        for col in columns:\n",
    "            data[col] = data[col].str.lower().replace(\"\\\\n\", \".\")\n",
    "        return data\n",
    "\n",
    "    data = pd.read_csv(path)\n",
    "    data = data.rename(columns=rename_dict)\n",
    "    data = data.drop(drop_list, axis=1)\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index()\n",
    "    data = data.drop(\"index\", axis=1)\n",
    "    data = normalize(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenization(data, col):\n",
    "    # Lists to store the results during the calculations\n",
    "    sentences = []\n",
    "    lengths_of_sentences = []\n",
    "\n",
    "    # Initialize names for columns\n",
    "    col_name = col + \"_sent_tok\"\n",
    "    col_name_length = col + \"_num_of_sent\"\n",
    "\n",
    "    # Iterating through the data row for row splitting the text in sentences and counting them\n",
    "    for entry in tqdm(data[col]):\n",
    "        col_sentences = nltk.sent_tokenize(entry)\n",
    "        sentences.append(col_sentences)\n",
    "        lengths_of_sentences.append(len(col_sentences))\n",
    "    \n",
    "    # Insert results to dataframe\n",
    "    data[col_name] = sentences\n",
    "    data[col_name_length] = lengths_of_sentences\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hallo, ich bin Niklas.', 'Wer genau bist du?', 'Wie kommst du hier her?', 'Ich hoffe, es geht dir gut!']\n",
      "['Ja hi, mir gehts gut!', 'Und dir?']\n"
     ]
    }
   ],
   "source": [
    "for entry in [\"Hallo, ich bin Niklas. Wer genau bist du? Wie kommst du hier her? Ich hoffe, es geht dir gut!\", \"Ja hi, mir gehts gut! Und dir?\"]:\n",
    "    sentence = nltk.sent_tokenize(entry)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization(data, col):\n",
    "    # Lists to store the results during the calculations\n",
    "    words = []\n",
    "    length_of_words = []\n",
    "\n",
    "    # Initialize names for columns\n",
    "    col_name = col + \"_word_tok\"\n",
    "    col_name_length = col + \"_num_of_word\"\n",
    "\n",
    "    # Iterating through the data row for row splitting the text in words and counting them\n",
    "    for i in tqdm(range(len(data))):\n",
    "        word_list = nltk.regexp_tokenize(data[col][i], pattern='\\w+')\n",
    "        words.append(word_list)\n",
    "        length_of_words.append(len(word_list))\n",
    "\n",
    "    # Insert results to dataframe\n",
    "    data[col_name] = words\n",
    "    data[col_name_length] = length_of_words\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_lemming(data, col):\n",
    "    # Initialize Stemmer and Lemmatizer\n",
    "    pst = nltk.PorterStemmer()\n",
    "    wlem = nltk.WordNetLemmatizer()\n",
    "\n",
    "    # Lists to store results\n",
    "    stems = []\n",
    "    lemms = []\n",
    "\n",
    "    # Initialize names for columns\n",
    "    col_name_stems = col + \"_stems\"\n",
    "    col_name_lemms = col + \"_lemms\"\n",
    "\n",
    "    # Iterating through the data row for row creating stems and lemms\n",
    "    for i in tqdm(range(len(data))):\n",
    "        stem_cache = []\n",
    "        lemm_cache = []\n",
    "\n",
    "        for c in data[col][i]:\n",
    "            stem_cache.append(pst.stem(c))\n",
    "            lemm_cache.append(wlem.lemmatize(c))\n",
    "        \n",
    "        stems.append(stem_cache)\n",
    "        lemms.append(lemm_cache)\n",
    "\n",
    "    # Insert results to dataframe\n",
    "    data[col_name_stems] = stems\n",
    "    data[col_name_lemms] = lemms\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_count_and_removal(data, col, language=\"english\"):\n",
    "\n",
    "    stoplist = nltk.stopwords.words(language)\n",
    "\n",
    "    number_of_stopwords = []\n",
    "    text_without_stopwords = []\n",
    "\n",
    "    col_name_number_of_stopwords = col + \"_num_of_stopwords\"\n",
    "    col_name_without_stopwords = col + \"_without_stopwords\"\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        no_of_words = 0\n",
    "        without_stopwords = []\n",
    "        for word in data[col][i]:\n",
    "            if word in stoplist:\n",
    "                no_of_words += 1\n",
    "            else:\n",
    "                without_stopwords.append(word)\n",
    "        \n",
    "        text_without_stopwords.append(without_stopwords)\n",
    "        number_of_stopwords.append(no_of_words)\n",
    "\n",
    "    data[col_name_number_of_stopwords] = number_of_stopwords\n",
    "    data[col_name_without_stopwords] = text_without_stopwords\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_dict(data, without_stopwords=True, with_stopwords=False):\n",
    "\n",
    "    if without_stopwords:\n",
    "        word_dict_without_stopwords = {}\n",
    "        \n",
    "    if with_stopwords:\n",
    "        word_dict_with_stopwords = {}\n",
    "\n",
    "    stopword_list = stopwords.words('english')\n",
    "    for i in tqdm(range(len(data))):\n",
    "        if without_stopwords:\n",
    "            for word in literal_eval(data[\"lemma_list\"][i]):\n",
    "                if word not in [\",\", \".\", \")\", \"(\", \"{\", \"}\", \"[\", \"]\", \":\", \";\", \"\\\"\\\"\", \"...\", \"I\", \"-PRON-\", \"-\", \"'\", \"'s\", \"urllink\"] and word not in stopword_list:\n",
    "                    if word in word_dict_without_stopwords.keys():\n",
    "                        word_dict_without_stopwords[word] += 1\n",
    "                    else:\n",
    "                        word_dict_without_stopwords[word] = 1\n",
    "\n",
    "        if with_stopwords:\n",
    "            for word in literal_eval(data[\"word_tokenize\"][i]):\n",
    "                if word not in [\",\", \".\", \")\", \"(\", \"{\", \"}\", \"[\", \"]\", \":\", \";\", \"\\\"\\\"\", \"...\", \"I\", \"-PRON-\", \"-\", \"'\"]:\n",
    "                    if word in word_dict_with_stopwords.keys():\n",
    "                        word_dict_with_stopwords[word] += 1\n",
    "                    else:\n",
    "                        word_dict_with_stopwords[word] = 1\n",
    "\n",
    "    if without_stopwords and with_stopwords:\n",
    "        return word_dict_without_stopwords, word_dict_with_stopwords\n",
    "    elif without_stopwords:\n",
    "        return word_dict_without_stopwords\n",
    "    elif with_stopwords:\n",
    "        return word_dict_with_stopwords\n",
    "    else:\n",
    "        print(\"Nothing to return selected\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_specific_words(dictionary, limit=10, save_fig=True, save_name=\"standard_save_name\"):\n",
    "    data_as_df = pd.DataFrame(data={\"word\": dictionary.keys(), \"number\": dictionary.values()})\n",
    "    sorted_df = data_as_df.sort_values(by=\"number\", ascending=False)\n",
    "\n",
    "    figure = sb.barplot(data=sorted_df[:limit], x=\"word\", y=\"number\", palette=\"deep\")\n",
    "    figure.set_xlabel(\"Words\")\n",
    "    figure.set_ylabel(\"Appearances of the word\")\n",
    "\n",
    "    if save_fig:\n",
    "        fig = figure.get_figure()\n",
    "        fig.savefig(\"images/gender/\" + str(save_name + \".jpg\"))\n",
    "\n",
    "    return fig.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_without_stopwords_all = create_word_dict(data, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumbers(data):\n",
    "    data[\"num_nouns\"] = 0\n",
    "    data[\"num_verbs\"] = 0\n",
    "    data[\"num_conjs\"] = 0\n",
    "    num_nouns, num_verbs, num_conjs = [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        nouns, verbs, conjs = 0, 0, 0\n",
    "        for elem in literal_eval(data[\"pos_list\"][i]):\n",
    "            #print(elem)\n",
    "            if elem == \"NOUN\":\n",
    "                nouns += 1\n",
    "            elif elem == \"VERB\":\n",
    "                verbs += 1\n",
    "            elif elem == \"CCONJ\":\n",
    "                conjs += 1\n",
    "        num_nouns.append(nouns)\n",
    "        num_verbs.append(verbs)\n",
    "        num_conjs.append(conjs)\n",
    "\n",
    "    data[\"num_nouns\"] = num_nouns\n",
    "    data[\"num_verbs\"] = num_verbs\n",
    "    data[\"num_conjs\"] = num_conjs\n",
    "\n",
    "    return data\n",
    "\n",
    "def getRatios(data):\n",
    "    data[\"ratio_nouns\"] = data.apply(lambda row: (row[\"num_nouns\"] / row[\"word_count\"]), axis=1)\n",
    "    data[\"ratio_verbs\"] = data.apply(lambda row: (row[\"num_verbs\"] / row[\"word_count\"]), axis=1)\n",
    "    data[\"ratio_conjs\"] = data.apply(lambda row: (row[\"num_conjs\"] / row[\"word_count\"]), axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getNumbers(data)\n",
    "data = getRatios(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cde137ca4d604021dfeee5cc69f15444c7734737e8b71c16850c523803c8f980"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
