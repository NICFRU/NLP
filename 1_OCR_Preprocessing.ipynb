{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "import cv2\n",
    "import pytesseract as pt\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ca. 190 min!!\n",
    "### Insert full path of input directory containing the election programs as .pdf file \n",
    "input_dir = Path(\"C:/DHBW WWI DS(A) Studium/S5/Natural Language Processing/NLP_Projekt/NLP/input/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstelle einzelne Bilder aus PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_2_images(input_dir=input_dir):\n",
    "    pdf_counter = 1\n",
    "    ### Iterate through pdf input directory and process only PDF files via .pdf extension\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            print(f\"Processing PDF file {pdf_counter}: {file}\")\n",
    "            pdf_counter += 1\n",
    "\n",
    "            ### Create entire filepath for each PDF file\n",
    "            pdf_file_path = input_dir/file\n",
    "            fn = file.split(\".\")[0]\n",
    "\n",
    "            ### Create a sub directory inside the \"input\" directory for each PDF\n",
    "            #############################################\n",
    "            sub_dir_path = input_dir/fn\n",
    "            ### Check if sub_dir already exists, if not --> create sub_dir for each PDF\n",
    "            if not os.path.exists(sub_dir_path):\n",
    "                os.mkdir(os.path.join(sub_dir_path))\n",
    "                print(f\"New SUB_DIR created: {sub_dir_path}\")\n",
    "            else:\n",
    "                print(f\"SUB_DIR already exists: {sub_dir_path}\")\n",
    "\n",
    "            ### Generate image file in sub_dir for each page in PDF file\n",
    "            #############################################\n",
    "            pages = convert_from_path(pdf_file_path, dpi=500)\n",
    "            for page_index, page in enumerate(pages):\n",
    "                ### Name schema of the image filenames\n",
    "                img_fn = f\"{fn}_Page_{str(page_index)}\"+\".png\"\n",
    "                ### Check if image file already exists\n",
    "                if not os.path.exists(sub_dir_path/img_fn):\n",
    "                    print(f\"Saving {fn} PDF page {page_index}\")\n",
    "                    page.save(sub_dir_path/img_fn, \"PNG\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"File already exists --> SKIP\")\n",
    "                    continue\n",
    "    print(\"PROCESSING FINISHED!\")\n",
    "\n",
    "pdf_2_images(input_dir=input_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstelle eine .txt Datei mit ausgelesenem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raw string extraction as .txt file of the image files combined per PDF/sub_dir\n",
    "def get_txt_from_images(input_dir=input_dir):\n",
    "    ### Create a list of the sub directories for later iteration\n",
    "    sub_dir_list = []\n",
    "    for dir in os.listdir(input_dir):\n",
    "        if (\".pdf\" not in dir) & (\".txt\" not in dir) & (\".xlsx\" not in dir) & (\".csv\" not in dir):\n",
    "            sub_dir_list.append(dir)\n",
    "        else:\n",
    "            continue\n",
    "    print(f\"List of Sub_DIRs: \\n{sub_dir_list}\")\n",
    "\n",
    "    ### Iterate over the list of sub directories and generate the sub_dir path\n",
    "    for sub_dir in sub_dir_list:\n",
    "        sub_dir_path = input_dir/sub_dir\n",
    "\n",
    "        ### Iterating through folder would lead to incorrect order 1, 10, 2, 3 --> Therefore iterate based on the filename numbering\n",
    "        ### Generate filename_structure by removing the number of the page (REMOVE --> _0.png)\n",
    "        fn_structure = os.listdir(sub_dir_path)[0]\n",
    "        fn_structure = fn_structure.split(\"_\")[0:-1]\n",
    "        fn_structure = \"_\".join(fn_structure)\n",
    "\n",
    "        ### .txt filepath\n",
    "        txt_file = Path(input_dir/f\"{sub_dir}.txt\")\n",
    "        with open(txt_file, \"a\", encoding=\"utf-8\") as output_file:\n",
    "\n",
    "            ### Generate the correct order for extraction \n",
    "            ### Iterating through folder would lead to incorrect order 1, 10, 2, 3 --> Therefore iterate based on the filename numbering\n",
    "            for img_no in range(0 , len(os.listdir(sub_dir_path))):\n",
    "                ### use the fn_structure for each party and generate filenames via iterating image numbers\n",
    "                img_file = fn_structure+f\"_{img_no}.png\"\n",
    "                print(f\"Extracting text from image: {img_file}\")\n",
    "\n",
    "                ## Recognize the text as string in image using pytesserct\n",
    "                ### German language package used \"deu\"\n",
    "                text = pt.image_to_string(\n",
    "                Image.open(sub_dir_path/img_file), \n",
    "                lang=\"deu\", \n",
    "                )\n",
    "                output_file.write(text)\n",
    "            print(f\"Save text into .txt file: {txt_file}\")\n",
    "\n",
    "    print(\"Text Extraction FINISHED!\")\n",
    "\n",
    "get_txt_from_images(input_dir=input_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstelle eine bereinigte .csv Datei mit ausgelesenem Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DF cleaning\n",
    "def process_df(df):\n",
    "    ### Entferne manche cols vom DF --> [level], [block_num], [par_num]\n",
    "    ### Wähle explizit die pytesseract cols, die benötigt werden und verkleinere das DF\n",
    "    col_lst = ['file', 'page_num', 'line_num', 'word_num', 'left', 'top', 'width', 'height', 'conf', 'text']\n",
    "    dfc = df[col_lst].copy()\n",
    "\n",
    "    ### Entferne alle Wordobjekte mit einer OCR Konfidenz < 80 --> unbekannt ob 70 optimal ist\n",
    "    dfc = dfc[dfc[\"conf\"] > 70].reset_index(drop=True)\n",
    "\n",
    "    ### Vergrößere das DF mit dem rechten Grenzwert eine Wordobjects [right] und dem Abstand zum nächsten Wordobjekt [delta]\n",
    "    right_border_list = []\n",
    "    delta_list = []\n",
    "    for index, rows in dfc.iterrows():\n",
    "            ### [right] = [Left] + [width] --> rechte Grenze eines Wordobjektes im DF\n",
    "            right_border_value = dfc.at[index, 'left'] + dfc.at[index, 'width']\n",
    "            right_border_list.append(right_border_value)\n",
    "            while index < dfc.index.max():\n",
    "                ### Wenn [right] > [left][+1] --> dann muss das nächste Wortobjekt im DF das letzte Wortobjekt der Zeile sein\n",
    "                ### Abstand [delta] = 0 zeigt an, dass es kein folgendes Wortobjekt in der Zeile gibt\n",
    "                if right_border_value >= dfc.at[index+1, 'left']:\n",
    "                    delta_value = 0\n",
    "                else:\n",
    "                    delta_value = dfc.at[index+1, 'left'] - right_border_value\n",
    "                delta_list.append(delta_value)\n",
    "                break\n",
    "    delta_list.append(0)\n",
    "    dfc.insert(6, 'right', right_border_list)  \n",
    "    dfc.insert(7, 'delta', delta_list)\n",
    "\n",
    "    return dfc\n",
    "\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "\n",
    "### Dataframe extraction as .csv file of the image files combined per PDF/sub_dir\n",
    "def get_df_from_images(input_dir=input_dir):\n",
    "\n",
    "    sub_dir_list = []\n",
    "    for dir in os.listdir(input_dir):\n",
    "        if (\".pdf\" not in dir) & (\".txt\" not in dir) & (\".xlsx\" not in dir) & (\".csv\" not in dir):\n",
    "            sub_dir_list.append(dir)\n",
    "        else:\n",
    "            continue\n",
    "    print(f\"List of Sub_DIRs: \\n{sub_dir_list}\")\n",
    "\n",
    "    ### Initiate MASTER_DF with all parties\n",
    "    master_df = pd.DataFrame()\n",
    "\n",
    "    ### Iterate over the list of sub directories and generate the sub_dir path\n",
    "    for sub_dir in sub_dir_list:\n",
    "        sub_dir_path = input_dir/sub_dir\n",
    "\n",
    "        ### Iterating through folder would lead to incorrect order 1, 10, 2, 3 --> Therefore iterate based on the filename numbering\n",
    "        ### Generate filename_structure by removing the number of the page (REMOVE --> _0.png)\n",
    "        fn_structure = os.listdir(sub_dir_path)[0]\n",
    "        fn_structure = fn_structure.split(\"_\")[0:-1]\n",
    "        fn_structure = \"_\".join(fn_structure)\n",
    "\n",
    "        ### Initiate an empty DF for each sub_dir\n",
    "        comb_df = pd.DataFrame()\n",
    "        ### Generate the correct order for extraction \n",
    "        ### Iterating through folder would lead to incorrect order 1, 10, 2, 3 --> Therefore iterate based on the filename numbering\n",
    "        for img_no in range(0 , len(os.listdir(sub_dir_path))):\n",
    "            ### use the fn_structure for each party and generate filenames via iterating image numbers\n",
    "            img_fname = fn_structure+f\"_{img_no}.png\"\n",
    "            img_fpath = sub_dir_path/img_fname\n",
    "            print(f\"Extracting DF from image: {img_fpath}\")\n",
    "\n",
    "            ### Read images and create pytesseract DF with coordinates for ecach image and combine for each sub_dir/PDF\n",
    "            img_file = cv2.imread(str(img_fpath), 1)\n",
    "            sub_df   = pt.image_to_data(\n",
    "                            ### German language package used \"deu\"\n",
    "                            img_file, lang=\"deu\",\n",
    "                            ### Pytesseract page segmentation mode (--psm 6) -> assume a single uniform block of text. \n",
    "                            ### https://ai-facets.org/tesseract-ocr-best-practices/\n",
    "                            config= r'--oem 1 --psm 6' ,\n",
    "                            output_type = pt.Output.DATAFRAME\n",
    "                            )\n",
    "            ### REMOVE the pytesseract page_num (always 1)\n",
    "            sub_df = sub_df.drop(columns=[\"page_num\"], axis=1)\n",
    "            ### ADD the correct page_num into the column for each sub_DF and insert at first position\n",
    "            sub_df.insert(0, \"page_num\", [img_no for i in range(0, len(sub_df))]) # image no which counts based on the length of the sub_dir --> MAYBE Change...\n",
    "            comb_df = pd.concat([comb_df, sub_df])\n",
    "\n",
    "        ### ADD the sub_dir name and insert at first position\n",
    "        comb_df.insert(0, \"file\", [sub_dir for i in range(0, len(comb_df))])\n",
    "\n",
    "        ### Säubere das DF mit Hilfe process_df() Funktion mit Hilfe der Konfidenz Werte bei der OCR (conf > 70)\n",
    "        comb_df = process_df(comb_df)\n",
    "        \n",
    "        ### Save separated DF for each sub_dir\n",
    "        comb_df.to_csv(Path(input_dir/f\"{sub_dir}.csv\"), index=False)\n",
    "        print(f\"Save DF into .csv file:\")\n",
    "\n",
    "        ### CONCATENATE the combined party_df into one MASTER_DF\n",
    "        master_df = pd.concat([master_df, comb_df])\n",
    "\n",
    "    ### SAVE and return the full Master_DF with all parties\n",
    "    master_df.to_csv(Path(input_dir/f\"MASTER.csv\"), index=False)\n",
    "    print(\"PROCESSING FINISHED!\")\n",
    "    return(master_df)\n",
    "    \n",
    "get_df_from_images(input_dir=input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(\"C:/DHBW WWI DS(A) Studium/S5/Natural Language Processing/NLP_Projekt/NLP/input/Master.csv\"))\n",
    "\n",
    "def create_bbx_images(input_dir=input_dir, df=df):\n",
    "\n",
    "    ### 1) Create list of the different files/parties\n",
    "    file_lst = df[\"file\"].unique().tolist()\n",
    "    \n",
    "    ### 2) Iterate over list an create Sub_DF for one file/party\n",
    "    for file in file_lst:\n",
    "        print(f\"Processing file {file}\")\n",
    "\n",
    "        ### 3) Create directory inside \"input\" directory to save the BBX images\n",
    "        file_ext = file+\"_BBX_Image\"\n",
    "        bbx_path = input_dir/file_ext\n",
    "        ### Check if sub_dir already exists, if not --> create sub_dir for each PDF\n",
    "        if not os.path.exists(bbx_path):\n",
    "            os.mkdir(os.path.join(bbx_path))\n",
    "            print(f\"New SUB_DIR created: {bbx_path}\")\n",
    "        else:\n",
    "            print(f\"SUB_DIR already exists: {bbx_path}\")\n",
    "\n",
    "        ### 4) Create Sub_DF with values of only one PDF file/party\n",
    "        sub_df = df[df[\"file\"]==file]\n",
    "\n",
    "        ### 5) Create list of the different pages in the file\n",
    "        page_lst = sub_df[\"page_num\"].unique().tolist()\n",
    "\n",
    "        ### 6) Iterate over page_list\n",
    "        for page in page_lst:\n",
    "            print(f\"Processing Page: {page}\")\n",
    "\n",
    "            ### 7) Grab the respective Image file\n",
    "            img = cv2.imread(str(input_dir)+f\"{file}\\\\{file}_Page_{page}.png\", 1)\n",
    "\n",
    "            ### 8) Create SubSub_DF with the values for each page\n",
    "            subsub_df = sub_df[sub_df[\"page_num\"]==page]\n",
    "\n",
    "            ### 9) Iterate over SubSub_DF and save the respective coordinates\n",
    "            for index, rows in subsub_df.iterrows():\n",
    "                (left, top, width, height) = subsub_df.at[index, 'left'], subsub_df.at[index, 'top'], subsub_df.at[index, 'width'], subsub_df.at[index, 'height']\n",
    "\n",
    "                #### 10) Draw rectangles and extend the rectangles slightly --> BBX Images\n",
    "                cv2.rectangle(img, (left, top), (left-5 + width+10, top-5 + height+10), (0, 0, 0), 10)\n",
    "\n",
    "            try: \n",
    "                ### 11) After finishing the iteration save the BBX Image in the respective directory\n",
    "                cv2.imwrite(str(bbx_path)+f\"{file}_BBX_Page_{page}.png\", img)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    print(\"FINISHED! BBX Images completely created!\")\n",
    "\n",
    "\n",
    "create_bbx_images(input_dir=input_dir, df=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nicht funktionaler Code zum Zusammenfügen der einzelnen Textobjekte mit \"Two Column Text Dateien\" (Linke, FDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdf = pd.read_csv(\"C:\\\\DHBW WWI DS(A) Studium\\\\S5\\\\Natural Language Processing\\\\NLP_Projekt\\\\input\\\\AFD_Wahlprogramm_2021_fragmented.csv\")\n",
    "\n",
    "def merge_sentences_from_df(df):\n",
    "    ### Mache eine Kopie des übergebenen DFs\n",
    "    dfc = df.copy()\n",
    "\n",
    "    ### Sortiere das DF --> 1) Seite, 2) Zeile, 3) linke Koordinate\n",
    "    dfc = dfc.sort_values([\"page_num\", \"line_num\", \"left\"], ascending=[True, True, True]).reset_index(drop=True)\n",
    "\n",
    "    for index, row in dfc.iterrows():\n",
    "        ### Abbruchbedingung der Iteration, weil auf den Nachfolgender verwiesen wird, daher Abbruch beim vorletzten Objekt\n",
    "        if index == dfc.index[-1]:\n",
    "            break\n",
    "        else:\n",
    "            ### Bei \".\" oder \"!\" oder \"?\" soll ein Merge von zwei Wortobjekten durchgeführt werden\n",
    "            if (\".\" not in dfc.at[index, \"text\"]) and (\"!\" not in dfc.at[index, \"text\"]) and (\"?\" not in dfc.at[index, \"text\"]):\n",
    "                ### Merge der Wortobjekte\n",
    "                dfc.loc[index+1, 'text'] = dfc.loc[index, 'text'] + ' ' + dfc.loc[index+1, 'text']\n",
    "                ### Änderung der Daten nach dem Merge --> Übernehme Daten des ersten Wortobjektes\n",
    "                dfc.loc[index+1, 'left'] = dfc.loc[index, 'left']\n",
    "                dfc.loc[index+1, 'line_num'] = dfc.loc[index, 'line_num']\n",
    "\n",
    "                ### Drop des ersten Wortobjektes nach dem Merge\n",
    "                dfc = dfc.drop(index)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return dfc\n",
    "\n",
    "# testdf_merged = merge_sentences_from_df(testdf)\n",
    "# testdf_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test = tdf1[19:100].reset_index(drop=True)\n",
    "# test = tdf1[0:50].reset_index(drop=True)\n",
    "\n",
    "# ####\n",
    "# ####\n",
    "# #### FIRST ITERATION\n",
    "# ####\n",
    "# ####\n",
    "# test1 = test.copy()\n",
    "# for index, row in test1.iterrows():\n",
    "#     if index == test1.index[-1]:\n",
    "#         break\n",
    "#     else:\n",
    "#         if (test1.at[index, 'delta'] <= 50 and test1.at[index, 'delta'] >= 10) and (\".\" not in test1.at[index, \"text\"]) and (\"!\" not in test1.at[index, \"text\"]) and (\"?\" not in test1.at[index, \"text\"]):\n",
    "#             ### Merge Wortobjekte\n",
    "#             test1.loc[index+1, 'text'] = test1.loc[index, 'text'] + ' ' + test1.loc[index+1, 'text']\n",
    "#             ### Übernahme die Daten des ersten Wortobjektes --> NICHT [delta] !!!!\n",
    "#             test1.loc[index+1, 'left'] = test1.loc[index, 'left']\n",
    "#             test1.loc[index+1, 'line_num'] = test1.loc[index, 'line_num']\n",
    "#             # test3.loc[index+1, 'word_num'] = test3.loc[index, 'word_num']\n",
    "#             # test3.loc[index+1, 'top'] = test3.loc[index, 'top']\n",
    "#             # test3.loc[index+1, 'right'] = test3.loc[index, 'right']\n",
    "#             # test3.loc[index+1, 'width'] = test3.loc[index, 'width']\n",
    "#             # test3.loc[index+1, 'height'] = test3.loc[index, 'height']\n",
    "#             # test3.loc[index+1, 'conf'] = test3.loc[index, 'conf']\n",
    "#             test1 = test1.drop(index)\n",
    "#         # elif (test3.at[index, 'delta'] == 0) and (\".\" not in test.at[index, \"text\"]) and (\"!\" not in test.at[index, \"text\"]) and (\"?\" not in test.at[index, \"text\"]):\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "# test1 = test1.sort_values([\"page_num\", \"line_num\", \"left\"], ascending=[True, True, True]).reset_index(drop=True)\n",
    "# test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2 = test1.copy()\n",
    "\n",
    "\n",
    "# for index, row in test2.iterrows():\n",
    "#     if index == test2.index[-1]:\n",
    "#         break\n",
    "#     else:\n",
    "#         # if (\".\" not in test2.at[index, \"text\"]) and (\"!\" not in test2.at[index, \"text\"]) and (\"?\" not in test2.at[index, \"text\"]):\n",
    "#         #     continue\n",
    "\n",
    "#         ### FIRST COLUMN in TEXT --> [delta] > 50 bedeutet, dass es ein nachfolgendes Wortobjekt in derselben Zeile\n",
    "#         if (test2.at[index, \"delta\"] > 50):\n",
    "#             nxt_line_num = test2.at[index, \"line_num\"] + 1\n",
    "#             ### Sub DF mit allen Wortobjekten der nächsten Zeile\n",
    "#             sub_df = test2[test2[\"line_num\"] == nxt_line_num]\n",
    "#             ### Iteriere über das Sub DF\n",
    "#             for sub_index, sub_row in sub_df.iterrows():\n",
    "#                 if sub_df.at[sub_index, \"left\"] < test2.at[index, \"left\"]:\n",
    "                    \n",
    "#                     ### ?????\n",
    "#                     test2.loc[sub_index, 'text'] = test2.loc[index, 'text'] + ' ' + test1.loc[sub_index, 'text']\n",
    "#                     break\n",
    "\n",
    "#                 else:\n",
    "#                     continue\n",
    "#             test2 = test2.drop(index)\n",
    "\n",
    "\n",
    "#         elif (test2.at[index, \"delta\"] == 0):\n",
    "#             nxt_line_num = test2.at[index, \"line_num\"] + 1\n",
    "#             ### Sub DF mit allen Wortobjekten der nächsten Zeile\n",
    "#             sub_df = test2[test2[\"line_num\"] == nxt_line_num]\n",
    "#             ### Iteriere über das Sub DF\n",
    "#             for sub_index, sub_row in sub_df.iterrows():\n",
    "#                 if (sub_df.at[sub_index, \"left\"] < test2.at[index, \"left\"]):\n",
    "#                 # if (sub_df.at[sub_index, \"left\"] >= test2.at[index, \"left\"]-20) and (sub_df.at[sub_index, \"left\"] <= test2.at[index, \"left\"]+20):\n",
    "                    \n",
    "#                     ### ?????\n",
    "#                     test2.loc[sub_index, 'text'] = test2.loc[index, 'text'] + ' ' + test1.loc[sub_index, 'text']\n",
    "#                     break\n",
    "\n",
    "#                 else:\n",
    "#                     continue\n",
    "#             test2 = test2.drop(index)\n",
    "\n",
    "#         else:\n",
    "#             continue\n",
    "        \n",
    "\n",
    "# test2 = test2.sort_values([\"page_num\", \"line_num\", \"left\"], ascending=[True, True, True]).reset_index(drop=True)\n",
    "# test2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0fd9740fc1352dc3b578dd222ff57d104255ee8b6e99126526451087bda250"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
