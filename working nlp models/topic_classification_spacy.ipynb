{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR= 'raw'\n",
    "data_names = os.listdir(DATA_DIR)\n",
    "data_names = [name[:-4] for name in data_names if name != \".DS_Store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_yaml_file(file_path):\n",
    "    # reads the yml files as a dictionary, were each topic is a key and the values are a list of elements\n",
    "    with open(file_path, \"r\", encoding='UTF-8') as stream:\n",
    "        yaml_dict = yaml.safe_load(stream)\n",
    "        return yaml_dict\n",
    "\n",
    "topics = load_yaml_file('topic_modeling/topic_g.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_markdown_file(file_path):\n",
    "    with open(file_path, \"r\", encoding='UTF-8') as stream:\n",
    "        markdown_str = stream.read()\n",
    "        return markdown_str\n",
    "\n",
    "def _add_sentence_to_list(sentence: str, sentences_list):\n",
    "    \"\"\"\n",
    "    Add a sentence to the list of sentences.\n",
    "    Args:\n",
    "        sentence (str):\n",
    "            Sentence to be added.\n",
    "        sentences (List[str]):\n",
    "            List of sentences.\n",
    "    \"\"\"\n",
    "    while sentence.startswith(\" \"):\n",
    "        # remove leading space\n",
    "        sentence = sentence[1:]\n",
    "    if all(c in punctuation for c in sentence) or len(sentence) == 1:\n",
    "        # skip sentences with only punctuation\n",
    "        return\n",
    "    sentences_list.append(sentence)\n",
    "\n",
    "def get_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Get sentences from a text.\n",
    "    Args:\n",
    "        text (str):\n",
    "            Text to be processed.\n",
    "    Returns:\n",
    "        List[str]:\n",
    "            List of sentences.\n",
    "    \"\"\"\n",
    "    # get the paragraphs\n",
    "    text=   re.sub(\" \\d+\\n\", \".\", text)\n",
    "    text=   re.sub(\"\\n\\d+\", \" \", text)\n",
    "    text=   re.sub(\"\\n\", \" \", text)\n",
    "    text=   re.sub(\"\\d+.\", \"\", text)\n",
    "    paragraphs = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', text)\n",
    "    paragraphs = [p for p in paragraphs if p != \"\"]\n",
    "    # get the sentences from the paragraphs\n",
    "    sentences = list()\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph.startswith(\"#\"):\n",
    "            _add_sentence_to_list(paragraph, sentences)\n",
    "            continue\n",
    "        prev_sentence_idx = 0\n",
    "        for idx in range(len(paragraph)):\n",
    "            if idx + 1 < len(paragraph):\n",
    "                if (paragraph[idx] == \".\" and not paragraph[idx + 1].isdigit()) or (\n",
    "                    paragraph[idx] in \"!?\"\n",
    "                ):\n",
    "                    sentence = paragraph[prev_sentence_idx : idx + 1]\n",
    "                    _add_sentence_to_list(sentence, sentences)\n",
    "                    prev_sentence_idx = idx + 1\n",
    "            else:\n",
    "                sentence = paragraph[prev_sentence_idx:]\n",
    "                _add_sentence_to_list(sentence, sentences)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def get_topical_sentences(\n",
    "    sentences, topics, df_y=0\n",
    ") :\n",
    "\n",
    "    \"classifies the content based on the frequency of the occurring words of the classes\"\n",
    "    sent_df=[]\n",
    "    topical_sentences = dict()\n",
    "    topics_list=[]\n",
    "    for topic in topics:\n",
    "        topics_list.append(topic)\n",
    "        topical_sentences[topic] = list()\n",
    "        #topical_sentences[f'{topic}_num'] = list()\n",
    "    for sentence in sentences:\n",
    "        topic_list=[]\n",
    "        for topic in topics:\n",
    "            topic_num = 0\n",
    "            if any(str(topical_word) in str(sentence.lower()) for topical_word in topics[topic]):\n",
    "                for  topical_word in topics[topic]:\n",
    "                        if str(topical_word) in str(sentence.lower()):\n",
    "                            topic_num+=1\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                topic_num=0\n",
    "            topic_list.append(topic_num)\n",
    "        \n",
    "        topical_sentences[topics_list[max(range(len(topic_list)), key=topic_list.__getitem__)]].append(sentence)\n",
    "        if df_y:\n",
    "            sent_df.append([sentence,topics_list[max(range(len(topic_list)), key=topic_list.__getitem__)]])\n",
    "    if df_y:\n",
    "        return pd.DataFrame(data=sent_df,columns=['text','topic'])\n",
    "\n",
    "    return topical_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\",disable=['parser', 'ner','tagger'])\n",
    "\n",
    "def text_lemma(lsit):\n",
    "    liste=[]\n",
    "    doc = nlp(lsit)\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            liste.append(token.lemma_.lower())\n",
    "\n",
    "    return ' '.join(liste)\n",
    "\n",
    "\n",
    "def list_lemma(lsit):\n",
    "    liste=[]\n",
    "    string=' '.join(lsit)\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    doc = nlp(str(string))\n",
    "    for token in tqdm(doc):\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            liste.append(token.lemma_.lower())\n",
    "    return list(dict.fromkeys(liste)) \n",
    "topics = load_yaml_file('topic_modeling/topic_g.yml')\n",
    "\n",
    "for topic in topics.keys():\n",
    "    topics[topic]=list_lemma(topics[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_topical_sentences(data_names, topics):\n",
    "    for element in list(filter(None, data_names)):\n",
    "        program_txt = load_markdown_file(f\"raw/{element}.txt\")\n",
    "        sentences = get_sentences(program_txt)\n",
    "        sent = []\n",
    "        for text in tqdm(sentences):\n",
    "            sent.append(text_lemma(text))\n",
    "        df1 = get_topical_sentences(sent, topics,1)\n",
    "        df1.to_csv(f'{element}_topic_class.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automated_topical_sentences(data_names, topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 10 2022, 13:17:42) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
